{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate GARCH(1,1) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def simulate_garch(T, omega_1, omega_2, omega_3):\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    r = np.zeros(T)\n",
    "    h = np.zeros(T)\n",
    "    r[0] = np.random.normal(0, np.sqrt(omega_1 / (1 - omega_2 - omega_3)))\n",
    "    h[0] = omega_1 / (1 - omega_2 - omega_3)\n",
    "    \n",
    "    for t in range(1, T):\n",
    "        h[t] = omega_1 + omega_2 * r[t-1]**2 + omega_3 * h[t-1]\n",
    "        r[t] = np.random.normal(0, np.sqrt(h[t]))\n",
    "        \n",
    "    return r, h\n",
    "\n",
    "# Parameters (example values)\n",
    "omega_1 = 0.1\n",
    "omega_2 = 0.1\n",
    "omega_3 = 0.8\n",
    "\n",
    "# Simulate data\n",
    "T = 1000  # Number of time points\n",
    "r, h = simulate_garch(T, omega_1, omega_2, omega_3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log-Likelihood & Log-Posterior Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The likelihood function for the GARCH model parameters $\\omega_1, \\omega_2, \\omega_3$ given returns $r$ is defined as:\n",
    "$$\n",
    "l(\\omega_1, \\omega_2, \\omega_3 \\mid r) \\propto \\prod_{t=1}^T h_t^{-\\frac{1}{2}} \\exp\\left(-\\frac{1}{2} \\sum_{t=1}^T \\frac{r_t^2}{h_t}\\right)\n",
    "$$\n",
    "\n",
    "The posterior distribution, incorporating independent truncated Normal priors for the parameters, is given by:\n",
    "$$\n",
    "\\pi(\\omega_1, \\omega_2, \\omega_3 \\mid r) \\propto \\exp\\left(-\\frac{1}{2} \\left(\\frac{\\omega_1^2}{\\sigma^2(\\omega_1)} + \\frac{\\omega_2^2}{\\sigma^2(\\omega_2)} + \\frac{\\omega_3^2}{\\sigma^2(\\omega_3)}\\right)\\right) \\prod_{t=1}^T h_t^{-\\frac{1}{2}} \\exp\\left(-\\frac{1}{2} \\sum_{t=1}^T \\frac{r_t^2}{h_t}\\right)\n",
    "$$\n",
    "\n",
    "This gives us both the log-likelihood expression and the log-posterior expression:\n",
    "$$\n",
    "\\log l(\\omega_1, \\omega_2, \\omega_3 \\mid r) = -\\frac{1}{2} \\sum_{t=1}^T \\left( \\log h_t + \\frac{r_t^2}{h_t}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\log \\pi(\\omega_1, \\omega_2, \\omega_3 \\mid r) = -\\frac{1}{2} \\left(\\frac{\\omega_1^2}{\\sigma^2(\\omega_1)} + \\frac{\\omega_2^2}{\\sigma^2(\\omega_2)} + \\frac{\\omega_3^2}{\\sigma^2(\\omega_3)}\\right) -\\frac{1}{2} \\sum_{t=1}^T \\left( \\log h_t + \\frac{r_t^2}{h_t}\\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(r, omega_1, omega_2, omega_3):\n",
    "    T = len(r)\n",
    "    h = np.zeros(T)\n",
    "    h[0] = omega_1 / (1 - omega_2 - omega_3) if (omega_2 + omega_3) < 1 else 100000  # large number for stability\n",
    "    \n",
    "    log_lik = 0\n",
    "    for t in range(1, T):\n",
    "        h[t] = omega_1 + omega_2 * r[t-1]**2 + omega_3 * h[t-1]\n",
    "        log_lik -= 0.5 * (np.log(h[t]) + r[t]**2 / h[t])\n",
    "    \n",
    "    return log_lik\n",
    "\n",
    "def log_posterior(r, omega_1, omega_2, omega_3, sigma_1, sigma_2, sigma_3):\n",
    "    log_prior = -0.5 * (omega_1**2 / sigma_1**2 + omega_2**2 / sigma_2**2 + omega_3**2 / sigma_3**2)\n",
    "    return log_likelihood(r, omega_1, omega_2, omega_3) + log_prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the RWMH Sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Follow the guidelines given in that experiment to implement a random walk Metropo-\n",
    "lis sampler that targets the posterior distribution of a GARCH model. For the\n",
    "data, you can use simulated data at first, and then look at the same type of real\n",
    "data (log-returns computed from exchange rates) in a second time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_garch(T, omega_1, omega_2, omega_3):\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    r = np.zeros(T)\n",
    "    h = np.zeros(T)\n",
    "    h[0] = omega_1 / (1 - omega_2 - omega_3)\n",
    "    r[0] = np.random.normal(0, np.sqrt(h[0]))\n",
    "\n",
    "    \n",
    "    for t in range(1, T):\n",
    "        h[t] = omega_1 + omega_2 * r[t-1]**2 + omega_3 * h[t-1]\n",
    "        r[t] = np.random.normal(0, np.sqrt(h[t]))\n",
    "        \n",
    "    return r, h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters (example values)\n",
    "omega_1 = 0.1\n",
    "omega_2 = 0.1\n",
    "omega_3 = 0.8\n",
    "\n",
    "# Simulate data\n",
    "T = 1000  # Number of time points\n",
    "r, h = simulate_garch(T, omega_1, omega_2, omega_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(r, omega_1, omega_2, omega_3):\n",
    "    T = len(r)\n",
    "    h = np.zeros(T)\n",
    "    h[0] = omega_1 / (1 - omega_2 - omega_3) if (omega_2 + omega_3) < 1 else 100000  # large number for stability\n",
    "    \n",
    "    log_lik = 0\n",
    "    for t in range(1, T):\n",
    "        h[t] = omega_1 + omega_2 * r[t-1]**2 + omega_3 * h[t-1]\n",
    "        log_lik -= 0.5 * (np.log(h[t]) + r[t]**2 / h[t])\n",
    "    \n",
    "    return log_lik\n",
    "\n",
    "def log_posterior(r, omega_1, omega_2, omega_3, sigma_1, sigma_2, sigma_3):\n",
    "    log_prior = -0.5 * (omega_1**2 / sigma_1**2 + omega_2**2 / sigma_2**2 + omega_3**2 / sigma_3**2)\n",
    "    return log_likelihood(r, omega_1, omega_2, omega_3) + log_prior\n",
    "\n",
    "def proposal(omega_old, sigma_proposal ): # function used in the MH algorithm that will \"propose\" the new parameter (new omega here)\n",
    "    # omega_old is the list containing the old omegas, we will use a normal probability distribution to determine the new ones\n",
    "    omega = np.array([0.0,0.0,0.0])\n",
    "    omega[0] = np.random.normal(loc= omega_old[0], scale = sigma_proposal)\n",
    "    omega[1] = np.random.normal(loc= omega_old[1], scale = sigma_proposal)\n",
    "    omega[2] = np.random.normal(loc= omega_old[2], scale = sigma_proposal)\n",
    "\n",
    "    return omega\n",
    "\n",
    "def metropolis_hasting_algorithm(iterations, returns, sigma_proposal, initial_omega, sigma_omega):\n",
    "    # Let's initialize our variable\n",
    "    acceptation = np.array([False]*iterations) # we will trough the iterations accept or not the change of value of omega\n",
    "    omega = initial_omega\n",
    "    omega_sampling = [omega]\n",
    "    # Let's apply the algorithm trough all the iterations\n",
    "    for t in range(iterations):\n",
    "        # We propose a candidate for omegas\n",
    "        omega_new = proposal(omega, sigma_proposal)\n",
    "        # We calculate the acceptance alpha which is ratio of the prior of the new omega over the old omega\n",
    "        alpha = log_posterior(returns,omega_new[0],omega_new[1],omega_new[2],sigma_omega[0],sigma_omega[1],sigma_omega[2]) - log_posterior(returns,omega[0],omega[1],omega[2],sigma_omega[0],sigma_omega[1],sigma_omega[2])\n",
    "        # We compare alpha with 1 and an uniform random number\n",
    "        u = np.random.uniform(0,1)\n",
    "        if alpha > 0:  # reminder that alpha is a log here so we don't compare with 1 but with 0\n",
    "            acceptation[t] = True\n",
    "            omega = omega_new\n",
    "        if alpha < 0:\n",
    "            if alpha > np.log(u): # we compare to log u and not u\n",
    "                acceptation[t]= True\n",
    "                omega = omega_new # we take the new omega\n",
    "        omega_sampling.append(omega)\n",
    "    acceptance_rate = np.sum(acceptation)/len(acceptation)*100\n",
    "    return acceptance_rate, omega_sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example on simulated data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we are going to test few things.\n",
    "First, if we get metropolis_omega near of our initial_omega that gave the return.\n",
    " Secondly, we will check if the f_tilde function allows to reduce the variance of the estimators while keeping the same expected value\n",
    " Finally, we will check the unbiasedness of our estimators that has been proven in one of the article\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "initial_omega = [0.3, 0.3, 0.3]\n",
    "true_omega = [0.6, 0.7, 0.3]  # These are the true parameters we want to estimate\n",
    "sigma_omega = [9, 9, 9]  # Standard deviations for the priors\n",
    "\n",
    "# Simulate data\n",
    "returns, _ = simulate_garch(900, *true_omega)\n",
    "\n",
    "# Metropolis-Hastings settings\n",
    "iterations = 5000\n",
    "burn_in = 500\n",
    "sigma_proposal = 0.01\n",
    "\n",
    "# Execute the Metropolis-Hastings algorithm\n",
    "acceptance_rate, omega_samples = metropolis_hasting_algorithm(iterations, returns, sigma_proposal, initial_omega, sigma_omega)\n",
    "\n",
    "# We extract the parameter samples\n",
    "omega_samples = np.array(omega_samples)  # Convert list of samples into an array for easier slicing\n",
    "w1_samples = omega_samples[:, 0]\n",
    "w2_samples = omega_samples[:, 1]\n",
    "w3_samples = omega_samples[:, 2]\n",
    "\n",
    "# We plot the parameter trajectories\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(w1_samples[burn_in:], label='ω1')\n",
    "plt.plot(w2_samples[burn_in:], label='ω2')\n",
    "plt.plot(w3_samples[burn_in:], label='ω3')\n",
    "plt.legend(fontsize=20)\n",
    "plt.xlabel('Iterations after burn-in', fontsize=20)\n",
    "plt.ylabel('Parameter values', fontsize=20)\n",
    "plt.title('Trace Plot of GARCH Parameters', fontsize=20)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As previously discussed, we expect that \n",
    "$$\\frac{1}{n}\\sum_{i=1}^n f(X_i) \\to \\int f \\, d\\pi \\quad \\text{almost surely as } n \\to \\infty.$$ \n",
    "In this instance, we define the identity function as $f$: $f([\\omega_1,\\omega_2,\\omega_3]) = [\\omega_1,\\omega_2,\\omega_3]$. Consequently, we will verify whether the empirical means of these three sequences approach the actual values of the parameters $\\omega_1$, $\\omega_2$, and $\\omega_3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the average values post burn-in\n",
    "expected_w1 = np.average(w1_samples[burn_in:])\n",
    "expected_w2 = np.average(w2_samples[burn_in:])\n",
    "expected_w3 = np.average(w3_samples[burn_in:])\n",
    "\n",
    "# Display the calculated mean values\n",
    "print(\"Calculated Mean Values:\")\n",
    "print(f\"Expected ω1: {expected_w1}\")\n",
    "print(f\"Expected ω2: {expected_w2}\")\n",
    "print(f\"Expected ω3: {expected_w3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our w1,w2,w3 implemented in the garch_simulation were : [0.1, 0.2, 0.7], therefore we have almost the same figures which means that our algorithm works well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a seaborn style to enhance aesthetics\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "fig, axs = plt.subplots(3, figsize=(12, 18))\n",
    "\n",
    "# Function to plot a histogram with matplotlib, with enhanced presentation\n",
    "def custom_hist(data, bins, color, label, ax):\n",
    "    counts, edges, _ = ax.hist(data, bins=bins, color=color, edgecolor='black', alpha=0.75)\n",
    "    ax.set_xlabel(label, fontsize=14)\n",
    "    ax.set_ylabel('Count', fontsize=14)\n",
    "    return counts, edges\n",
    "\n",
    "# Plot histograms with detailed customization\n",
    "_, edges1 = custom_hist(w1_samples[burn_in:], 30, 'steelblue', 'Histogram of ω1', axs[0])\n",
    "axs[0].set_title('Posterior Distribution of ω1', fontsize=16)\n",
    "\n",
    "_, edges2 = custom_hist(w2_samples[burn_in:], 50, 'forestgreen', 'Histogram of ω2', axs[1])\n",
    "axs[1].set_title('Posterior Distribution of ω2', fontsize=16)\n",
    "\n",
    "_, edges3 = custom_hist(w3_samples[burn_in:], 35, 'firebrick', 'Histogram of ω3', axs[2])\n",
    "axs[2].set_title('Posterior Distribution of ω3', fontsize=16)\n",
    "\n",
    "# Fine-tuning the plot layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that these histograms of the posterior distribution of w1,w2 w3 are almost histograms from normal distribution. However, they are not perfectly centered with the empirical expected value of omega but they are indeed near of them. We could also see it with boxplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are now doing to develop a function that will iterate the metropolis_hasting_algorithm in order to make statistical test and study on the distributions of the omega\n",
    "\n",
    "def mean_metropolis_algorithm(iterations1):\n",
    "    expected_w1_list = []\n",
    "    expected_w2_list = []\n",
    "    expected_w3_list = []\n",
    "    for i in range(iterations1):\n",
    "        results = metropolis_hasting_algorithm(iterations, returns, sigma_proposal, initial_omega, sigma_omega)\n",
    "\n",
    "        # Check if results structure is as expected\n",
    "        if not isinstance(results, tuple) or len(results) < 2:\n",
    "            raise ValueError(\"The results from the Metropolis-Hasting algorithm do not match the expected format.\")\n",
    "\n",
    "        omega_samples = results[1]  # Assuming the second item is the list of omega samples\n",
    "\n",
    "        # Prepare lists to hold the current iteration values for w1, w2, w3\n",
    "        w1_samples = []\n",
    "        w2_samples = []\n",
    "        w3_samples = []\n",
    "\n",
    "        # Extract samples depending on whether they are scalar or array-like\n",
    "        for sample in omega_samples:\n",
    "            if np.isscalar(sample):  # Handling cases where the sample might be a single scalar value\n",
    "                w1_samples.append(sample)  # Assuming scalar results pertain to w1 or need clarification on which omega they relate to\n",
    "            else:  # Otherwise, handle as arrays\n",
    "                w1_samples.append(sample[0])\n",
    "                w2_samples.append(sample[1])\n",
    "                w3_samples.append(sample[2])\n",
    "\n",
    "        # Calculate means after burn-in period\n",
    "        w1_expected = np.mean(w1_samples[burn_in:])\n",
    "        w2_expected = np.mean(w2_samples[burn_in:])\n",
    "        w3_expected = np.mean(w3_samples[burn_in:])\n",
    "        \n",
    "        expected_w1_list.append(w1_expected)\n",
    "        expected_w2_list.append(w2_expected)\n",
    "        expected_w3_list.append(w3_expected)\n",
    "        \n",
    "    return [expected_w1_list, expected_w2_list, expected_w3_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example on real date (exchange rate like on the first article on zero variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "initial_omega = [0.3, 0.3, 0.3]\n",
    "true_omega = [0.6, 0.7, 0.3]  # These are the true parameters we want to estimate\n",
    "sigma_omega = [9, 9, 9]  # Standard deviations for the priors\n",
    "\n",
    "# Simulate data\n",
    "returns, _ = simulate_garch(900, *true_omega)\n",
    "\n",
    "# Metropolis-Hastings settings\n",
    "iterations = 3000 # less iterations than before\n",
    "burn_in = 300 # smaller burn_in too\n",
    "sigma_proposal = 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_met_alg = mean_metropolis_algorithm(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_met_alg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So now, as we said before we are going to check if the f_tilde function allows to reduce the variance of the estimators while keeping the same expected value\n",
    "# Thanks to the mean_metropolis_algorithm we can get the variance of our estimators\n",
    "\n",
    "def variance_estimators(mean_met_alg):\n",
    "    return statistics.variance(mean_met_alg[0]), statistics.variance(mean_met_alg[1]), statistics.variance(mean_met_alg[2])\n",
    "\n",
    "# So the variance without the method used in the article is \n",
    "\n",
    "print(\"The variance without the method used in the article is:\")\n",
    "print(f\"Variance de ω1,w2,w3 : {variance_estimators(mean_met_alg)}\")\n",
    "\n",
    "# We will see the variance reduction in the next questions\n",
    "\n",
    "# Now, let's check the unbiasedness of our estimators that has been proven in one of the article.\n",
    "\n",
    "def bias_estimators(mean_met_alg):\n",
    "    return statistics.mean(mean_met_alg[0]) - true_omega[0], statistics.mean(mean_met_alg[1]) - true_omega[1], statistics.mean(mean_met_alg[2]) - true_omega[2]\n",
    "\n",
    "print(f\"The bias of the estimators are : {bias_estimators(mean_met_alg)}\")\n",
    "\n",
    "# There is still some bias but it is very small and it may be explicated by the few numbers of iterations of the metropolis_hasting_algorithm, so like it was said in the paper, there is the unbiasedness of the estimators\n",
    "\n",
    "# To end this part we can finally check the mean squared error of the estimators\n",
    "\n",
    "mse = {}\n",
    "\n",
    "# We calculate the MSE for each parameter\n",
    "for i in range(1,4):\n",
    "    mse[i] = statistics.mean([(omega - true_omega[i-1])**2 for omega in mean_met_alg[i-1]])\n",
    "\n",
    "print(mse)\n",
    "\n",
    "# The mse for both parameters are very small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's do the example on real data as it is asked in the end of the first question**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def fetch_ecb_data(series_id, start_date, end_date):\n",
    "    # Define the URL template for accessing ECB data\n",
    "    url = f\"https://sdw-wsrest.ecb.europa.eu/service/data/EXR/D.{series_id}.EUR.SP00.A\"\n",
    "\n",
    "    # Set up headers to specify the desired response format as 'generic' XML\n",
    "    headers = {\n",
    "        'Accept': 'text/xml'\n",
    "    }\n",
    "    params = {\n",
    "        'startPeriod': start_date,\n",
    "        'endPeriod': end_date\n",
    "    }\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "\n",
    "    # We parse the XML response\n",
    "    root = etree.fromstring(response.content)\n",
    "    ns = {'ns': 'http://www.sdmx.org/resources/sdmxml/schemas/v2_1/message',\n",
    "          'common': 'http://www.sdmx.org/resources/sdmxml/schemas/v2_1/common',\n",
    "          'generic': 'http://www.sdmx.org/resources/sdmxml/schemas/v2_1/data/generic'}\n",
    "\n",
    "    # Then we extract data points\n",
    "    data = []\n",
    "    for series in root.xpath('.//generic:Series', namespaces=ns):\n",
    "        for obs in series.xpath('.//generic:Obs', namespaces=ns):\n",
    "            time = obs.find('.//generic:ObsDimension', namespaces=ns).get('value')\n",
    "            value = obs.find('.//generic:ObsValue', namespaces=ns).get('value')\n",
    "            data.append({'Date': time, 'Rate': value})\n",
    "\n",
    "    # Convert list of dictionaries into a DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df['rate'] = pd.to_numeric(df['Rate'], errors='coerce')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use EUR to GBP exchanges rates on the year 2019 to 2022:\n",
    "df = fetch_ecb_data('GBP', '2019-01-01', '2022-12-31')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"rate\"] = (df[\"rate\"] - df[\"rate\"].shift(1)) / df[\"rate\"].shift(1)\n",
    "df = df.dropna()\n",
    "df = df.replace([np.inf, -np.inf], np.nan).dropna(subset=['rate'])\n",
    "df['rate'] = df['rate'].interpolate() # for the missing values\n",
    "rates = df[\"rate\"].to_numpy()*100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will do the same as in the first part of the question with simulated data, we will compare the garch model with the metropolis_hasting samplings\n",
    "\n",
    "# We fit a garch model with our real datas :\n",
    "import arch\n",
    "real_garch = arch.arch_model(rates, vol = 'GARCH', p=1, q=1) # GARCH(1,1)\n",
    "\n",
    "real_garch = real_garch.fit()\n",
    "\n",
    "print(real_garch.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we use our metropolis_hasting_algorithm with our real data, we use the same parameters as for the simulated data : ( the same thing as for simulated data)\n",
    "\n",
    "# Parameters\n",
    "initial_omega = [0.3, 0.3, 0.3]\n",
    "true_omega = [0.3, 0.4, 0.9]  # These are the true parameters we want to estimate\n",
    "sigma_omega = [9, 9, 9]  # Standard deviations for the priors\n",
    "\n",
    "# Simulate data\n",
    "returns = rates\n",
    "\n",
    "# Metropolis-Hastings settings\n",
    "iterations = 5000\n",
    "burn_in = 500\n",
    "sigma_proposal = 0.01\n",
    "\n",
    "# Execute the Metropolis-Hastings algorithm\n",
    "acceptance_rate, omega_samples = metropolis_hasting_algorithm(iterations, returns, sigma_proposal, initial_omega, sigma_omega)\n",
    "\n",
    "# We extract the parameter samples\n",
    "omega_samples = np.array(omega_samples)  # Convert list of samples into an array for easier slicing\n",
    "w1_samples = omega_samples[:, 0]\n",
    "w2_samples = omega_samples[:, 1]\n",
    "w3_samples = omega_samples[:, 2]\n",
    "\n",
    "# We plot the parameter trajectories\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(w1_samples[burn_in:], label='ω1')\n",
    "plt.plot(w2_samples[burn_in:], label='ω2')\n",
    "plt.plot(w3_samples[burn_in:], label='ω3')\n",
    "plt.legend(fontsize=20)\n",
    "plt.xlabel('Iterations after burn-in', fontsize=20)\n",
    "plt.ylabel('Parameter values', fontsize=20)\n",
    "plt.title('Trace Plot of GARCH Parameters', fontsize=20)\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_met_alg2 = mean_metropolis_algorithm(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's reuse the function we have implemented for the simulated datas :\n",
    "\n",
    "# Variance\n",
    "print(f\"Variance de ω1,w2,w3 : {variance_estimators(mean_met_alg2)}\")\n",
    "# Bias\n",
    "print(f\"The bias of the estimators are : {bias_estimators(mean_met_alg2)}\")\n",
    "# MSE\n",
    "mse = {}\n",
    "for i in range(1,4):\n",
    "    mse[i] = statistics.mean([(omega - true_omega[i-1])**2 for omega in mean_met_alg2[i-1]])\n",
    "print(mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have very small variance, bias and mse as in with simulated datas !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also check directly the mean of the estimators rather than the bias :\n",
    "\n",
    "def mean_estimators(mean_met_alg2):\n",
    "    return statistics.mean(mean_met_alg2[0]), statistics.mean(mean_met_alg2[1]), statistics.mean(mean_met_alg2[2])\n",
    "\n",
    "print(f\"The mean value of the estimators are : {mean_estimators(mean_met_alg2)}\") # Recall : true_omega_1 = 0. , true_omega_2 = 0.4, true_omega_3 = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have some very few difference between the expected empirical means and the true values. With real datas of EUR to GBP on 2019-2022, it works less than with our simulated datas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Computations to find Control Variates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient of the log-posterior $\\ln \\pi$ with respect to each parameter $ \\omega_i $ involves contributions from the prior and the likelihood:\n",
    "$$\n",
    "\\frac{\\partial \\ln \\pi}{\\partial \\omega_i} = -\\frac{\\omega_i}{\\sigma^2(\\omega_i)} - \\frac{1}{2} \\sum_{t=1}^T \\left( \\frac{1}{h_t} \\frac{\\partial h_t}{\\partial \\omega_i} - \\frac{r_t^2}{h_t^2} \\frac{\\partial h_t}{\\partial \\omega_i} \\right), \\quad i = 1, 2, 3\n",
    "$$\n",
    "\n",
    "The derivatives of the volatility equation $ h_t $ with respect to the parameters are defined as:\n",
    "$$\n",
    "\\frac{\\partial h_t}{\\partial \\omega_1} = \\frac{1 - \\omega_3^{t-1}}{1 - \\omega_3} \\quad \\text{(assuming $ \\omega_3 \\neq 1 $)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial h_t}{\\partial \\omega_2} = r_{t-1}^2 + \\omega_3 \\frac{\\partial h_{t-1}}{\\partial \\omega_2} \\quad \\text{for } t > 1 \\text{, and 0 else}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial h_t}{\\partial \\omega_3} = h_{t-1} + \\omega_3 \\frac{\\partial h_{t-1}}{\\partial \\omega_3} \\quad \\text{for } t > 1 \\text{, and 0 else}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Error in the article ? Should be $\\omega_3^t$:**\n",
    "\n",
    "*Objective:*\n",
    "Compute the derivative of $h_t= \\omega_1 + \\omega_2 r_{t-1}^2 + \\omega_3 h_{t-1}$ with respect to $\\omega_1$, $\\frac{\\partial h_t}{\\partial \\omega_1}$.\n",
    "\n",
    "*Initial Setup:*\n",
    "Since $\\omega_1$ is a constant addition at each time step $t$,\n",
    "$ \\frac{\\partial \\omega_1}{\\partial \\omega_1} = 1 $\n",
    "and since $r_{t-1}^2$ does not depend on $\\omega_1$,\n",
    "$ \\frac{\\partial (r_{t-1}^2)}{\\partial \\omega_1} = 0 $\n",
    "\n",
    "*Recursive Derivation:*\n",
    "Starting from the base case at $t=0$, assuming $h_0$ is either non-dependent on $\\omega_1$ or set by an initial condition (making it a constant):\n",
    "$ h_1 = \\omega_1 + \\omega_2 r_0^2 + \\omega_3 h_0 $ gives \n",
    "$ \\frac{\\partial h_1}{\\partial \\omega_1} = 1 $\n",
    "\n",
    "At $t=2$:\n",
    "$ h_2 = \\omega_1 + \\omega_2 r_1^2 + \\omega_3 h_1 $ gives\n",
    "$ \\frac{\\partial h_2}{\\partial \\omega_1} = 1 + \\omega_3 \\cdot \\frac{\\partial h_1}{\\partial \\omega_1} = 1 + \\omega_3 $\n",
    "\n",
    "Proceeding recursively,\n",
    "$ h_3 = \\omega_1 + \\omega_2 r_2^2 + \\omega_3 h_2 $ gives \n",
    "$ \\frac{\\partial h_3}{\\partial \\omega_1} = 1 + \\omega_3 \\cdot \\frac{\\partial h_2}{\\partial \\omega_1} = 1 + \\omega_3 + \\omega_3^2 $\n",
    "\n",
    "$$\\vdots$$\n",
    "\n",
    "$ \\frac{\\partial h_t}{\\partial \\omega_1} = 1 + \\omega_3 + \\omega_3^2 + \\ldots + \\omega_3^{t-1} $\n",
    "\n",
    "Summation as a geometric series for $\\omega_3 \\neq 1$ yields:\n",
    "$$ \\frac{\\partial h_t}{\\partial \\omega_1} = \\frac{1 - \\omega_3^t}{1 - \\omega_3} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_h_t_derivatives(r, omega_1, omega_2, omega_3):\n",
    "    T = len(r)\n",
    "    h = np.zeros(T)\n",
    "    grad_h_omega1 = np.zeros(T)\n",
    "    grad_h_omega2 = np.zeros(T)\n",
    "    grad_h_omega3 = np.zeros(T)\n",
    "\n",
    "    # Initial volatility\n",
    "    h[0] = omega_1  \n",
    "\n",
    "    # Compute h and its derivatives\n",
    "    for t in range(1, T):\n",
    "        h[t] = omega_1 + omega_2 * r[t-1]**2 + omega_3 * h[t-1]\n",
    "        \n",
    "        # Derivative of h_t with respect to omega_1\n",
    "        grad_h_omega1[t] = (1 - omega_3**(t)) / (1 - omega_3) if omega_3 != 1 else t  # Using the geometric series\n",
    "\n",
    "        # Derivative of h_t with respect to omega_2\n",
    "        grad_h_omega2[t] = r[t-1]**2 + omega_3 * grad_h_omega2[t-1]\n",
    "\n",
    "        # Derivative of h_t with respect to omega_3\n",
    "        grad_h_omega3[t] = h[t-1] + omega_3 * grad_h_omega3[t-1]\n",
    "\n",
    "    return h, grad_h_omega1, grad_h_omega2, grad_h_omega3\n",
    "\n",
    "def compute_log_posterior_gradients(r, omega_1, omega_2, omega_3, sigma_1, sigma_2, sigma_3):\n",
    "    h, grad_h_omega1, grad_h_omega2, grad_h_omega3 = compute_h_t_derivatives(r, omega_1, omega_2, omega_3)\n",
    "    T = len(r)\n",
    "    gradients = np.zeros(3)\n",
    "\n",
    "    # Compute the gradient of the log-posterior for each parameter\n",
    "    gradients[0] = -omega_1 / sigma_1**2 - 0.5 * np.sum((1 / h) * grad_h_omega1 - (r**2 / h**2) * grad_h_omega1)\n",
    "    gradients[1] = -omega_2 / sigma_2**2 - 0.5 * np.sum((1 / h) * grad_h_omega2 - (r**2 / h**2) * grad_h_omega2)\n",
    "    gradients[2] = -omega_3 / sigma_3**2 - 0.5 * np.sum((1 / h) * grad_h_omega3 - (r**2 / h**2) * grad_h_omega3)\n",
    "\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We suppose that P is a polynomial : $P(x) = \\sum_{j=1}^{3}a_jx_j $.  \n",
    "Our goal is to find the vector $a = (a_1, a_2, a_3)$ that minimizes the variance of $\\widetilde{f} = f + a^Tz$, where $z = -\\frac{1}{2}∇ lnπ(x)$.\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now show that it is possible to find the optimal a by doing a linear regression.\n",
    "\n",
    "$$V(\\widetilde{f}) = V(f+a^Tz) = \\mathbb{E}[(f+a^Tz)^2] - \\mathbb{E}[(f+a^Tz)]^2 = \\mathbb{E}[(f+a^Tz)^2] - \\mathbb{E}[f]^2 \\quad$$ \n",
    "The last equality comes from the fact that by construction $\\mathbb{E}[\\widetilde{f}] = \\mathbb{E}[f] $.\n",
    "\n",
    "We showed that $\\mathbb{E}[f]^2$ does not depend by $a$. \n",
    "So minimizing $V(\\widetilde{f})$ is equivalent to minimizing $\\mathbb{E}[(f+a^Tz)^2] = \\mathbb{E}[(f- a^T(-z))^2] $.  \n",
    "We recognize the minimization problem of a linear regression of $f$ on $-z$ so we know that $a = \\mathbb{E}[zz^T]^{-1}\\mathbb{E}[zf]$ as the article says.\n",
    "\n",
    "If $ \\mathbb{E}[f^2]<∞$ and $\\mathbb{E}[[ || -z || ]^2]<∞$ and $\\mathbb{E}[zz^T]$ is a non singular matrix then the OLS estimator is consistent (if observations are IID). So we will estimate $a$ by $â$ the OLS estimator.\n",
    "$$â = (\\sum_{t=1}^{T}z_tz_t^T)^{-1}(\\sum_{t=1}^{T}z_tf_t)$$\n",
    "\n",
    "ATTENTION PEUT ETRE QUE NOS  $(z_t,f_t)$ ne sont pas IID.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_a(f,z):\n",
    "    '''\n",
    "    Calculates the OLS estimator of the vector a\n",
    "\n",
    "    Args : \n",
    "        x (float): The point at which to evaluate the alternative function.\n",
    "        f (ndarray): Vector containing observed values of the original function f.\n",
    "        \n",
    "    Return : OLS estimator of the vector a\n",
    "    '''\n",
    "    model = LinearRegression()\n",
    "    model.fit(f, -z) #performs the linear regression of f on -z\n",
    "    a = model.coef_\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alternative_f(x,f,log_post_gradient):\n",
    "    '''\n",
    "    Calculates the value of the alternative function `f_tilde`\n",
    "    at a given point `x` based on the observed values of the original function `f` and log-posterior \n",
    "    gradient `log_post_gradient`.\n",
    "\n",
    "    Args:\n",
    "        x (float): The point at which to evaluate the alternative function.\n",
    "        f (ndarray): Vector containing observed values of the original function f.\n",
    "        log_post_gradient (ndarray): The log-posterior gradient.\n",
    "\n",
    "    Return:\n",
    "        float: The value of the alternative function `f_tilde` at point `x`.\n",
    "    '''\n",
    "    z = (-1/2) * log_post_gradient\n",
    "    a = optimal_a(f,z)\n",
    "    return f(x)+ np.dot(a.t,z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will test this method.\n",
    "As previously discussed, we expect that \n",
    "$$\\frac{1}{n}\\sum_{i=1}^n \\widetilde{f}(X_i)  = \\frac{1}{n}\\sum_{i=1}^n (f(X_i) + a^Tz(X_i)) \\to \\int \\widetilde{f}  \\, d\\pi \\quad \\text{almost surely as } n \\to \\infty.$$ \n",
    "$f$ is the same as in question 1: $f([\\omega_1,\\omega_2,\\omega_3]) = [\\omega_1,\\omega_2,\\omega_3]$. Consequently, we will verify whether the empirical means of these three sequences approach the actual values of the parameters $\\omega_1$, $\\omega_2$, and $\\omega_3$ and if the variance is lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters (example values)\n",
    "omega_1, omega_2, omega_3 =0.1, 0.1, 0.8\n",
    "sigma_1, sigma_2, sigma_3 = 9, 9, 9\n",
    "# Simulate data\n",
    "T = 1000  # Number of time points\n",
    "r, h = simulate_garch(T, omega_1, omega_2, omega_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we have\n",
    "log_post_gradient = compute_log_posterior_gradients(r, omega_1, omega_2, omega_3, sigma_1, sigma_2, sigma_3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1_expected = np.mean(alternative_f(w1_samples[burn_in:]))\n",
    "w2_expected = np.mean(alternative_f(w2_samples[burn_in:]))\n",
    "w3_expected = np.mean(alternative_f(w3_samples[burn_in:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute a \n",
    "#on peut oublier cette cellule\n",
    "#surement pas la meilleure methode, je vais faire un truc mieux avec une regression apres\n",
    "\n",
    "def optimal_a(r, omega_1, omega_2, omega_3, sigma_1, sigma_2, sigma_3):\n",
    "    gradients = compute_log_posterior_gradients(r, omega_1, omega_2, omega_3, sigma_1, sigma_2, sigma_3) \n",
    "    a = [0,0,0]\n",
    "    for t in range(0,gradients[0].size + 1):\n",
    "        z = (-1/2) * gradients[][t] \n",
    "        Sigma_matrix = np.dot(z,z.t)\n",
    "        sigma_zf = np.dot(z,f)\n",
    "        a = a + (-np.dot(np.linalg.inv(Sigma_matrix),sigma_zf)) \n",
    "\n",
    "return a/gradients[0].size #la moyenne des a \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
