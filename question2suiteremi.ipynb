{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We suppose that P is a polynomial : $P(x) = \\sum_{j=1}^{3}a_jx_j $.  \n",
    "Our goal is to find the vector $a = (a_1, a_2, a_3)$ that minimizes the variance of $\\widetilde{f} = f + a^Tz$, where $z = -\\frac{1}{2}∇ lnπ(x)$.\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now show that it is possible to find the optimal a by doing a linear regression.\n",
    "\n",
    "$$V(\\widetilde{f}) = V(f+a^Tz) = \\mathbb{E}[(f+a^Tz)^2] - \\mathbb{E}[(f+a^Tz)]^2 = \\mathbb{E}[(f+a^Tz)^2] - \\mathbb{E}[f]^2 \\quad$$ \n",
    "The last equality comes from the fact that by construction $\\mathbb{E}[\\widetilde{f}] = \\mathbb{E}[f] $.\n",
    "\n",
    "We showed that $\\mathbb{E}[f]^2$ does not depend by $a$. \n",
    "So minimizing $V(\\widetilde{f})$ is equivalent to minimizing $\\mathbb{E}[(f+a^Tz)^2] = \\mathbb{E}[(f- a^T(-z))^2] $.  \n",
    "We recognize the minimization problem of a linear regression of $f$ on $-z$ so we know that $a = \\mathbb{E}[zz^T]^{-1}\\mathbb{E}[zf]$ as the article says.\n",
    "\n",
    "If $ \\mathbb{E}[f^2]<∞$ and $\\mathbb{E}[[ || -z || ]^2]<∞$ and $\\mathbb{E}[zz^T]$ is a non singular matrix then the OLS estimator is consistent (if observations are IID). So we will estimate $a$ by $â$ the OLS estimator.\n",
    "$$â = (\\sum_{t=1}^{T}z_tz_t^T)^{-1}(\\sum_{t=1}^{T}z_tf_t)$$\n",
    "\n",
    "ATTENTION PEUT ETRE QUE NOS  $(z_t,f_t)$ ne sont pas IID.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_a(f,z):\n",
    "    '''\n",
    "    return : OLS estimator of the vector a\n",
    "    '''\n",
    "    model = LinearRegression()\n",
    "    model.fit(f, -z) #performs the linear regression of f on -z\n",
    "    a = model.coef_\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alternative_f(x,f,z):\n",
    "    '''\n",
    "    return value of alternative function f (f tilde) at point x\n",
    "    '''\n",
    "    a = optimal_a(f,z)\n",
    "    return f(x)+ np.dot(a.t,z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute a \n",
    "#surement pas la meilleure methode, je vais faire un truc mieux avec une regression apres\n",
    "\n",
    "def optimal_a(r, omega_1, omega_2, omega_3, sigma_1, sigma_2, sigma_3):\n",
    "    gradients = compute_log_posterior_gradients(r, omega_1, omega_2, omega_3, sigma_1, sigma_2, sigma_3) \n",
    "    a = [0,0,0]\n",
    "    for t in range(0,gradients[0].size + 1):\n",
    "        z = (-1/2) * gradients[][t] \n",
    "        Sigma_matrix = np.dot(z,z.t)\n",
    "        sigma_zf = np.dot(z,f)\n",
    "        a = a + (-np.dot(np.linalg.inv(Sigma_matrix),sigma_zf)) \n",
    "\n",
    "return a/gradients[0].size #la moyenne des a \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
