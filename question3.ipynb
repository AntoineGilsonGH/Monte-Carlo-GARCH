{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import truncnorm\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "import arch \n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import MultiTaskLasso\n",
    "from sklearn.linear_model import MultiTaskLassoCV"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **III)** Implementation of a method based on Lasso regression in the case of a high degree polynomial (large set if control variates) and comparison with the naive approach *(Question 3)*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When the number of control variates become too large, the linear regression approach of Step 2 may become too expensive. Let's explain why.**\n",
    "\n",
    "First of all, when we consider <polynomials, increasing by 1 the degree of the polynomial function increases a lot the number of control variates since, in a $d$-dimensionnal space, a polynomial of order $p$ provides $\\binom{d+p}{d}-1$ control variates according to the first paper. Therefore, for instance in our example where we are in a 3-dimensional space, we effectively have $\\binom{1+3}{1}-1 = \\binom{4}{1}-1 = 3$ control variates $z_1, z_2$ and $z_3$ for the first order polynomial case. For the 2 order polynomial case, we would have $z_1, z_2, z_3, z_1*z_2, z_3*z_3, z_1*z_3, z_1^2, z_2^2$ and $z_3^2$ which effectively corresponds to  $\\binom{2+3}{2}-1 = \\binom{5}{2}-1 = 9$ control variates. In the same way, we would have $\\binom{3+3}{3}-1 = \\binom{6}{3}-1 = 19$ for the 3rd order polynomial case. \n",
    "\n",
    "On the other side, the second paper states that the computation time of the OLS method is of the order $nm^2 + m^3 + nt$, where $n$ is the number of samples and $m$ the number of control variates. Indeed, when doing OLS, we need to compute $(𝑋′𝑋)^{−1}𝑋′𝑌$. In general, the complexity of the matrix product $AB$ of two matrices $A$ and $B$ with dimensions $a \\times b$ and $b \\times c$, respectively, can be expressed as $O(abc)$. Therefore, \n",
    "\n",
    "a) The matrix product $X'X$ has a complexity of $O(p^2n)$.\n",
    "\n",
    "b) The matrix-vector product $X'Y$ has a complexity of $O(pn)$.\n",
    "\n",
    "c) The inverse $(X'X)^{-1}$ has a complexity of $O(p^3)$.\n",
    "\n",
    "Thus, the overall complexity of these operations is $O(np^2 + p^3)$.\n",
    "\n",
    "As we saw that the number of controle variates increases rapidly when we increase the degree of the polynomial, this can therefore lead to a significant increase in the complexity of the computation when searching for the optimal coefficients by doing an OLS regression.\n",
    "That is why the second paper puts forward a solution to this problem which consists of using a Lasso regression to reduce the number of control variates used. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, in statistics and machine learning, LASSO (least absolute shrinkage and selection operator) is a regression analysis method that performs both variable selection and regularization in order to enhance the prediction accuracy and interpretability of the resulting statistical model.\n",
    "\n",
    "It is similar to the OLS but with an additional term which corresponds to lambda, an hyperparameter to find, times the norm 1 of the vector of coefficients. Therefore, in a LASSO regression, we search the parameters that satisfies this : $$\\displaystyle \\min_{\\beta \\in \\mathbb{R}^p} {\\frac{1}{N}} |y - X\\beta|_2^2 + \\lambda |\\beta|_1.$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, in statistics and machine learning, LASSO (least absolute shrinkage and selection operator) is a regression analysis method that performs both variable selection and regularization in order to enhance the prediction accuracy and interpretability of the resulting statistical model.\n",
    "\n",
    "It is similar to the OLS but with an additional term which corresponds to lambda, an hyperparameter to find, times the norm 1 of the vector of coefficients. Therefore, in a LASSO regression, we search the parameters that satisfies this : $$\\displaystyle \\min_{\\beta \\in \\mathbb{R}^p} {\\frac{1}{N}} |y - X\\beta|_2^2 + \\lambda |\\beta|_1.$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question, we are focusing on 2nd order polynomials. As we know that, for quadratic polynomials $P(x) = a^T\\times x + \\frac{1}{2}\\times x^TBx$ (formula of the paper), the re-normalized $\\tilde{f}$ is now:$\\tilde{f}(x) = f(x) - \\frac{1}{2}\\operatorname{tr}(B) + (a + Bx)^Tz$. Here, if we note $\\alpha_1$, $\\alpha_2$, $\\alpha_3$, $\\alpha_{12}$, $\\alpha_{13}$, $\\alpha_{23}$, $\\alpha_{11}$, $\\alpha_{22}$ and $\\alpha_{33}$ the coefficients of $z_1$, $z_2$, $z_3$,$z_1\\times z_2$, $z_1\\times z_3$, $z_2\\times z_3$, $z_1^2$, $z_2^2$ and $z_3^2$, then we have : $a = [\\alpha_1, \\alpha_2, \\alpha_3]$ and \n",
    "$ B = \\begin{pmatrix}\n",
    "\\alpha_{11} & \\alpha_{12} & \\alpha_{13} \\\\\n",
    "\\alpha_{21} & \\alpha_{22} & \\alpha_{23} \\\\\n",
    "\\alpha_{31} & \\alpha_{32} & \\alpha_{33} \\\\\n",
    "\\end{pmatrix}$ but with B symmetric so we only have 6 distinct coefficients in this matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_garch(omega, T):\n",
    "    h0 = omega[0]/(1 - omega[1] - omega[2])\n",
    "    std_h = [np.sqrt(h0)]\n",
    "    returns = [norm.rvs(loc = 0, scale = np.sqrt(h0))]\n",
    "    for i in range(1,T):\n",
    "        #formula of h_i defined in the GARCH(1,1)\n",
    "        h_i = omega[0] + omega[1]* returns[i-1]**2 + omega[2]* std_h[i-1]**2\n",
    "        std_h.append(np.sqrt(h_i))\n",
    "        #the returns are normally distributed with a lmean of 0 and a variance of h_i\n",
    "        returns.append(norm.rvs(loc = 0, scale = np.sqrt(h_i)))\n",
    "    return np.asarray(returns), std_h \n",
    "\n",
    "initial_omega = [0.3,0.3,0.3]\n",
    "\n",
    "returns = simulate_garch(omega = [0.1,0.2,0.7], T = 1000)[0]\n",
    "\n",
    "iterations = 2000\n",
    "burn_in = 1000\n",
    "std_prior = 10\n",
    "std_proposal = 0.01\n",
    "\n",
    "# Posterior desity function of the GARCH(1,1) model\n",
    "def posterior_density(returns, omega, std_prior):\n",
    "    n = len(returns)\n",
    "    variances = np.zeros(n)\n",
    "    variances[0] = np.var(returns)\n",
    "    for i in range(1, n):\n",
    "        variances[i] = omega[0] + omega[1] * returns[i-1]**2 + omega[2] * variances[i-1]\n",
    "    std = np.sqrt(variances)\n",
    "    log_likelihood = np.sum(norm.logpdf(returns, 0, std))\n",
    "    log_prior = np.log(prior(omega, std_prior)) \n",
    "    log_posterior = log_likelihood + log_prior\n",
    "    return log_posterior\n",
    "\n",
    "# Function that returns the previous one, it will be used for the gradient later\n",
    "def _post(omega):\n",
    "    return posterior_density(returns, omega, std_prior)\n",
    "\n",
    "def gradient(f, x):\n",
    "    \n",
    "    \"\"\"\n",
    "    It computes the gradient of function f at a point x.\n",
    "    f is a function with many variables.\n",
    "    x is the vector that that represents the point where the gradient is computed. \n",
    "    \"\"\"\n",
    "    h = 1e-6\n",
    "    gradient = np.zeros_like(x)\n",
    "    for i in range(x.size):\n",
    "        x_plus_h = x.copy()\n",
    "        x_plus_h[i] += h\n",
    "        gradient[i] = (f(x_plus_h) - f(x)) / h\n",
    "    return gradient"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
