{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import truncnorm\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "import arch \n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import MultiTaskLasso\n",
    "from sklearn.linear_model import MultiTaskLassoCV"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **III)** Implementation of a method based on Lasso regression in the case of a high degree polynomial (large set if control variates) and comparison with the naive approach *(Question 3)*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First of all, lets explain why, when the number of control variates become too large, the linear regression approach of Step 2 may become too expensive.**\n",
    "\n",
    "First of all, when we consider polynomials, increasing by 1 the degree of the polynomial function increases a lot the number of control variates since, in a $d$-dimensionnal space, a polynomial of order $p$ provides $\\binom{d+p}{d}-1$ control variates according to the first paper. \n",
    "\n",
    "Therefore, for instance in our example where we are in a 3-dimensional space, we effectively have $\\binom{1+3}{1}-1 = \\binom{4}{1}-1 = 3$ control variates that are $z_1, z_2$ and $z_3$ for the first order polynomial case. And, for the 2 order polynomial case, we would have $z_1, z_2, z_3, z_1*z_2, z_3*z_3, z_1*z_3, z_1^2, z_2^2$ and $z_3^2$ which effectively corresponds to  $\\binom{2+3}{2}-1 = \\binom{5}{2}-1 = 9$ control variates. In the same way, we would have $\\binom{3+3}{3}-1 = \\binom{6}{3}-1 = 19$ for the 3rd order polynomial case. \n",
    "\n",
    "On the other hand, the second paper states that the computation time of the OLS method is of the order $nm^2 + m^3 + nt$, where $n$ is the number of samples and $m$ the number of control variates. Indeed, when doing OLS, we need to compute $(ùëã‚Ä≤ùëã)^{‚àí1}ùëã‚Ä≤ùëå$. In general, the complexity of the matrix product $AB$ of two matrices $A$ and $B$ with dimensions $a \\times b$ and $b \\times c$, respectively, can be expressed as $O(abc)$. Therefore, \n",
    "\n",
    "a) The matrix product $X'X$ has a complexity of $O(p^2n)$.\n",
    "\n",
    "b) The matrix-vector product $X'Y$ has a complexity of $O(pn)$.\n",
    "\n",
    "c) The inverse $(X'X)^{-1}$ has a complexity of $O(p^3)$.\n",
    "\n",
    "Thus, the overall complexity of these operations is $O(np^2 + p^3)$.\n",
    "\n",
    "Since we observed a rapid escalation in the number of control variates with higher polynomial degrees, this consequently escalates computational complexity when seeking optimal coefficients through OLS regression. Hence, the second paper proposes a remedy: employing Lasso regression to diminish the count of control variates utilized."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, in statistics and machine learning, LASSO (least absolute shrinkage and selection operator) is a regression analysis method that performs both variable selection and regularization in order to enhance the prediction accuracy and interpretability of the resulting statistical model.\n",
    "\n",
    "It is similar to the OLS but with an additional term which corresponds to lambda, an hyperparameter to find, times the norm 1 of the vector of coefficients. Therefore, in a LASSO regression, we search the parameters that satisfies this : $$\\displaystyle \\min_{\\beta \\in \\mathbb{R}^p} {\\frac{1}{N}} |y - X\\beta|_2^2 + \\lambda |\\beta|_1.$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Therefore, we can adapt the method described in the second paper as the LSLasso method in order to deal with the issue of a large set of control variates. We are going to do the second order polynomial case here.**\n",
    "\n",
    "Indeed, the method that we are going to apply is:\n",
    "\n",
    "**1)** We are first going to find the 9 control variates based on the derivative of ln(posterior) as we did with the 3 control variates in II¬∞ but with a second order polynomial case. We use the gradient function as in question 2 in order to compute $z_1$, $z_2$ and $z_3$, and then, the next 6 control variates that we are going to use are $z_1\\times z_2$, $z_1\\times z_3$, $z_2\\times z_3$, $z_1^2$, $z_2^2$ and $z_3^2$. \n",
    "\n",
    "**2)** Then, we are going to do a LASSO Cross Validation (LassoCV) to regress the $f(w_i)$ on the vector of control variates $[z_i]$ as in Question 2 (but we have 9 control variates instead of 3 so 9 parameters to estimate for each $w_i$) in order to find the optimal hyperparameter $\\lambda$.\n",
    "\n",
    "**3)** Next, armed with the optimal hyperparameter $\\lambda$, we'll conduct LASSO Regression of $f(w_i) = w_i$ on $[z_i]$, employing subsampling to lower computation time. This approach aims to zero out certain coefficients, helping us identify which control variates to retain for the final OLS regression.\n",
    "\n",
    "**4)** With the control variates we kept (after LASSO), we are going to do our OLS regression to find the optimal parameters of the control variates.\n",
    "\n",
    "**5)** Ultimately, we can compute, for instance, the empirical expected value (mean) of each $\\tilde{f}(w_i)$ to determine if we achieve the same expected value as with only $f(w_i)$ but with reduced variance, fulfilling the objective of employing control variates."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***In this question, we are focusing on 2nd order polynomials.***\n",
    " \n",
    " As we know that for quadratic polynomials $P(x) = a^T\\times x + \\frac{1}{2}\\times x^TBx$ (as given in the paper), the re-normalized $\\tilde{f}$ is now:$\\tilde{f}(x) = f(x) - \\frac{1}{2}\\operatorname{tr}(B) + (a + Bx)^Tz$. \n",
    " \n",
    " Here, if we note $\\alpha_1$, $\\alpha_2$, $\\alpha_3$, $\\alpha_{12}$, $\\alpha_{13}$, $\\alpha_{23}$, $\\alpha_{11}$, $\\alpha_{22}$ and $\\alpha_{33}$ the coefficients of $z_1$, $z_2$, $z_3$,$z_1\\times z_2$, $z_1\\times z_3$, $z_2\\times z_3$, $z_1^2$, $z_2^2$ and $z_3^2$, then we have : $a = [\\alpha_1, \\alpha_2, \\alpha_3]$ and \n",
    "$ B = \\begin{pmatrix}\n",
    "\\alpha_{11} & \\alpha_{12} & \\alpha_{13} \\\\\n",
    "\\alpha_{21} & \\alpha_{22} & \\alpha_{23} \\\\\n",
    "\\alpha_{31} & \\alpha_{32} & \\alpha_{33} \\\\\n",
    "\\end{pmatrix}$ but with B symmetric so we only have 6 distinct coefficients in this matrix. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Step 0 :*** Let's remember the functions we'll have to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_garch(T, omega_1, omega_2, omega_3):\n",
    "    np.random.seed(423)  # For reproducibility\n",
    "    r = np.zeros(T)\n",
    "    h = np.zeros(T)\n",
    "    h[0] = omega_1 / (1 - omega_2 - omega_3)\n",
    "    r[0] = np.random.normal(0, np.sqrt(abs(h[0]))) # abs\n",
    "\n",
    "    \n",
    "    for t in range(1, T):\n",
    "        h[t] = omega_1 + omega_2 * r[t-1]**2 + omega_3 * h[t-1]\n",
    "        r[t] = np.random.normal(0, np.sqrt(abs(h[t]))) #abs\n",
    "        \n",
    "    return r, h\n",
    "\n",
    "def log_likelihood(r, omega_1, omega_2, omega_3):\n",
    "    T = len(r)\n",
    "    h = np.zeros(T)\n",
    "    h[0] = omega_1 / (1 - omega_2 - omega_3) if (omega_2 + omega_3) < 1 else 100000  # large number for stability\n",
    "    \n",
    "    log_lik = 0\n",
    "    for t in range(1, T):\n",
    "        h[t] = omega_1 + omega_2 * r[t-1]**2 + omega_3 * h[t-1]\n",
    "        log_lik -= 0.5 * (np.log(abs(h[t])) + r[t]**2 / h[t]) # abs\n",
    "    \n",
    "    return log_lik\n",
    "\n",
    "def log_posterior(r, omega_1, omega_2, omega_3, sigma_1, sigma_2, sigma_3):\n",
    "    log_prior = -0.5 * (omega_1**2 / sigma_1**2 + omega_2**2 / sigma_2**2 + omega_3**2 / sigma_3**2)\n",
    "    return log_likelihood(r, omega_1, omega_2, omega_3) + log_prior\n",
    "\n",
    "def proposal(omega_old, sigma_proposal ): # function used in the MH algorithm that will \"propose\" the new parameter (new omega here)\n",
    "    # omega_old is the list containing the old omegas, we will use a normal probability distribution to determine the new ones\n",
    "    omega = np.array([0.0,0.0,0.0])\n",
    "    omega[0] = np.random.normal(loc= omega_old[0], scale = sigma_proposal)\n",
    "    omega[1] = np.random.normal(loc= omega_old[1], scale = sigma_proposal)\n",
    "    omega[2] = np.random.normal(loc= omega_old[2], scale = sigma_proposal)\n",
    "\n",
    "    return omega\n",
    "\n",
    "def metropolis_hasting_algorithm(iterations, returns, sigma_proposal, initial_omega, sigma_omega):\n",
    "    # Let's initialize our variable\n",
    "    acceptation = np.array([False]*iterations) # we will through the iterations accept or not the change of value of omega\n",
    "    omega = initial_omega\n",
    "    omega_sampling = [omega]\n",
    "    # Let's apply the algorithm trough all the iterations\n",
    "    for t in range(iterations):\n",
    "        # We propose a candidate for omegas\n",
    "        omega_new = proposal(omega, sigma_proposal)\n",
    "        # We calculate the acceptance alpha which is ratio of the prior of the new omega over the old omega\n",
    "        alpha = log_posterior(returns,omega_new[0],omega_new[1],omega_new[2],sigma_omega[0],sigma_omega[1],sigma_omega[2]) - log_posterior(returns,omega[0],omega[1],omega[2],sigma_omega[0],sigma_omega[1],sigma_omega[2])\n",
    "        # We compare alpha with 1 and an uniform random number\n",
    "        u = np.random.uniform(0,1)\n",
    "        if alpha > 0:  # reminder that alpha is a log here so we don't compare with 1 but with 0\n",
    "            acceptation[t] = True\n",
    "            omega = omega_new\n",
    "        if alpha < 0:\n",
    "            if alpha > np.log(u): # we compare to log u and not u\n",
    "                acceptation[t]= False\n",
    "                omega = omega_new # we take the new omega\n",
    "        omega_sampling.append(omega)\n",
    "    acceptance_rate = np.sum(acceptation)/len(acceptation)*100\n",
    "    return acceptance_rate, omega_sampling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_h_t_derivatives(r, omega_1, omega_2, omega_3):\n",
    "    T = len(r)\n",
    "    h = np.zeros(T)\n",
    "    grad_h_omega1 = np.zeros(T)\n",
    "    grad_h_omega2 = np.zeros(T)\n",
    "    grad_h_omega3 = np.zeros(T)\n",
    "\n",
    "    # Initial volatility\n",
    "    h[0] = omega_1  \n",
    "\n",
    "    # Compute h and its derivatives\n",
    "    for t in range(1, T):\n",
    "        h[t] = omega_1 + omega_2 * r[t-1]**2 + omega_3 * h[t-1]\n",
    "        \n",
    "        # Derivative of h_t with respect to omega_1\n",
    "        grad_h_omega1[t] = (1 - omega_3**(t)) / (1 - omega_3) if omega_3 != 1 else t  # Using the geometric series\n",
    "\n",
    "        # Derivative of h_t with respect to omega_2\n",
    "        grad_h_omega2[t] = r[t-1]**2 + omega_3 * grad_h_omega2[t-1]\n",
    "\n",
    "        # Derivative of h_t with respect to omega_3\n",
    "        grad_h_omega3[t] = h[t-1] + omega_3 * grad_h_omega3[t-1]\n",
    "\n",
    "    return h, grad_h_omega1, grad_h_omega2, grad_h_omega3\n",
    "\n",
    "def compute_log_posterior_gradients(r, omega_1, omega_2, omega_3, sigma_1, sigma_2, sigma_3):\n",
    "    h, grad_h_omega1, grad_h_omega2, grad_h_omega3 = compute_h_t_derivatives(r, omega_1, omega_2, omega_3)\n",
    "    T = len(r)\n",
    "    gradients = np.zeros(3)\n",
    "\n",
    "    # Compute the gradient of the log-posterior for each parameter\n",
    "    gradients[0] = -omega_1 / sigma_1**2 - 0.5 * np.sum((1 / h) * grad_h_omega1 - (r**2 / h**2) * grad_h_omega1)\n",
    "    gradients[1] = -omega_2 / sigma_2**2 - 0.5 * np.sum((1 / h) * grad_h_omega2 - (r**2 / h**2) * grad_h_omega2)\n",
    "    gradients[2] = -omega_3 / sigma_3**2 - 0.5 * np.sum((1 / h) * grad_h_omega3 - (r**2 / h**2) * grad_h_omega3)\n",
    "\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_posterior_gradients_concatenator(r, sigma_1, sigma_2, sigma_3,w1_samples,w2_samples,w3_samples):\n",
    "    '''\n",
    "    This function computes the log posterior gradients at every points\n",
    "    '''\n",
    "    gradient_concat=[]\n",
    "    i = 0\n",
    "    for _ in r:\n",
    "        gradient_concat.append(compute_log_posterior_gradients(r, w1_samples[i],w2_samples[i],w3_samples[i], sigma_1, sigma_2, sigma_3))\n",
    "        i = i +1 \n",
    "    return np.array(gradient_concat)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's define a new function that will help us calculte the 6 missing control variates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_product(x,y):\n",
    "    result = []\n",
    "    # The 2 lists must be of same length\n",
    "    for i in range(len(x)):\n",
    "        product = x[i] * y[i]\n",
    "        result.append(product)\n",
    "    return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Step 1 :*** Compute the 6 new control variates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "initial_omega = [0.3, 0.3, 0.3]\n",
    "true_omega = [0.6, 0.7, 0.4]  # These are the true parameters we want to estimate\n",
    "sigma_omega = [9, 9, 9]  # Standard deviations for the priors\n",
    "\n",
    "# Metropolis-Hastings settings\n",
    "iterations = 5000\n",
    "burn_in = 1000\n",
    "sigma_proposal = 0.01\n",
    "\n",
    "# Parameters (example values)\n",
    "omega_1, omega_2, omega_3 =0.1, 0.1, 0.8\n",
    "sigma_1, sigma_2, sigma_3 = 9, 9, 9\n",
    "# Simulate data\n",
    "T = iterations + 1 # Number of time points\n",
    "r, h = simulate_garch(T, true_omega[0], true_omega[1], true_omega[2])\n",
    "\n",
    "# Execute the Metropolis-Hastings algorithm\n",
    "acceptance_rate, omega_samples = metropolis_hasting_algorithm(iterations, r, sigma_proposal, initial_omega, sigma_omega)\n",
    "\n",
    "# We extract the parameter samples\n",
    "omega_samples = np.array(omega_samples)  # Convert list of samples into an array for easier slicing\n",
    "w1_samples = omega_samples[:, 0]\n",
    "w2_samples = omega_samples[:, 1]\n",
    "w3_samples = omega_samples[:, 2]\n",
    "\n",
    "# We compute the log posterior gradients at every point\n",
    "log_post_gradient = compute_log_posterior_gradients_concatenator(r, sigma_1, sigma_2, sigma_3, w1_samples, w2_samples, w3_samples)\n",
    "\n",
    "# We compute the control variates (z_i)\n",
    "x1 = [elem[0] if isinstance(elem, np.ndarray) else elem[0] for elem in log_post_gradient]\n",
    "x2 = [elem[1] if isinstance(elem, np.ndarray) else elem[1] for elem in log_post_gradient]\n",
    "x3 = [elem[2] if isinstance(elem, np.ndarray) else elem[2] for elem in log_post_gradient]\n",
    "\n",
    "x1x2 = list_product(x1, x2)\n",
    "x1x3 = list_product(x1, x3)\n",
    "x2x3 = list_product(x2, x3)\n",
    "x1_square = list_product(x1, x1)\n",
    "x2_square = list_product(x2, x2)\n",
    "x3_square = list_product(x3, x3)\n",
    "\n",
    "# We store these 9 control variates in an array \n",
    "z = np.column_stack((x1, x2, x3, x1x2, x1x3, x2x3, x1_square, x2_square, x3_square))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.84557902e+03, 1.10675770e+04, 9.86898076e+03, ...,\n",
       "        1.47884780e+07, 1.22491262e+08, 9.73967812e+07],\n",
       "       [3.84557902e+03, 1.10675770e+04, 9.86898076e+03, ...,\n",
       "        1.47884780e+07, 1.22491262e+08, 9.73967812e+07],\n",
       "       [3.84557902e+03, 1.10675770e+04, 9.86898076e+03, ...,\n",
       "        1.47884780e+07, 1.22491262e+08, 9.73967812e+07],\n",
       "       ...,\n",
       "       [7.10645000e+00, 3.96999555e+02, 3.83220842e+02, ...,\n",
       "        5.05016316e+01, 1.57608647e+05, 1.46858214e+05],\n",
       "       [7.10645000e+00, 3.96999555e+02, 3.83220842e+02, ...,\n",
       "        5.05016316e+01, 1.57608647e+05, 1.46858214e+05],\n",
       "       [7.10645000e+00, 3.96999555e+02, 3.83220842e+02, ...,\n",
       "        5.05016316e+01, 1.57608647e+05, 1.46858214e+05]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We indeed obtained our new z, with 9 control variates for each iteration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Step 2 :*** Lasso cross validation to find the best hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.column_stack((w1_samples, w2_samples, w3_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We store our data in arrays after burn in \n",
    "Y = np.array(w[burn_in:])\n",
    "Z = np.array(z[burn_in:])\n",
    "\n",
    "# We also store especially w1,w2 and w3, it might be useful later\n",
    "Y1 = np.array(w1_samples[burn_in:])\n",
    "Y2 = np.array(w2_samples[burn_in:])\n",
    "Y3 = np.array(w3_samples[burn_in:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/antoine/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0007859554789024781, tolerance: 0.0006274315532178564\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/antoine/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0007701482585110719, tolerance: 0.0006274315532178564\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/antoine/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.000736520083183867, tolerance: 0.0006274315532178564\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal lambda: 0.9569567913835515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/antoine/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.319e-03, tolerance: 8.562e-04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "# We do a Lasso Cross Validation in order to find the optimal parameter lambda : step 2 of the method\n",
    "\n",
    "alphas = [0.0001, 0.0001, 0.001, 0.1, 1,1.5] # List of hyperparameters that will be tested through Cross Validation\n",
    "\n",
    "lasso_cv = LassoCV(cv=5) # √©valuer les performances de la r√©gression Lasso\n",
    "\n",
    "lasso_cv.fit(Z,Y1) # utilisation de la validation crois√©e, avec Z variable de controle, Y variable expliqu√©e\n",
    "\n",
    "print(\"Optimal lambda:\", lasso_cv.alpha_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Step 3 :*** Lasso regression using the parameters we found, and only using the most useful control variates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/antoine/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.610e-03, tolerance: 8.562e-04\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/antoine/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.828e-04, tolerance: 8.884e-05\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/antoine/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.507e-04, tolerance: 8.489e-05\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "# With the optimal lambda we just found, we do a Lasso regression to select the variables\n",
    "lasso_bestmodel = Lasso(alpha=lasso_cv.alpha_)\n",
    "lasso_bestmodel.fit(Z,Y)\n",
    "coeff_lasso = lasso_bestmodel.coef_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# probl√®me : ils sont quasi tous nuls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -3.39864865e-06, -4.07773124e-06,  8.17090660e-07,\n",
       "         1.95193871e-05, -5.08005446e-07,  1.55968049e-07],\n",
       "       [-0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "         7.64386893e-07, -1.04728616e-06,  4.41171283e-07,\n",
       "         1.74706330e-06, -7.42223873e-07,  3.04539106e-08],\n",
       "       [ 0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "         7.52965913e-08,  1.06785624e-06, -4.77198987e-07,\n",
       "        -2.74421484e-06,  4.83490314e-07, -2.40883307e-08]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeff_lasso"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that some coefficients have been set to 0 by Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_non_null(coeff_lasso): # the goal is to extract the rows of the Z matrix where the coefficients of each w_i are not zero\n",
    "\n",
    "    # Indexes of the non nul coefficients in the first, second and third column\n",
    "    nonnull_w1 = np.nonzero(coeff_lasso[:, 0])[0].tolist()\n",
    "    nonnull_w2 = np.nonzero(coeff_lasso[:, 1])[0].tolist()\n",
    "    nonnull_w3 = np.nonzero(coeff_lasso[:, 2])[0].tolist()\n",
    "\n",
    "    return nonnull_w1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_non_null(coeff_lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_extract_non_null(coeff_lasso): # the goal is to extract the rows of the Z matrix where the coefficients of each w_i are not zero\n",
    "\n",
    "    # Indexes of the non nul coefficients in the first, second and third column\n",
    "    nonnull_w1 = np.nonzero(coeff_lasso[:, 0])[0].tolist()\n",
    "    nonnull_w2 = np.nonzero(coeff_lasso[:, 1])[0].tolist()\n",
    "    nonnull_w3 = np.nonzero(coeff_lasso[:, 2])[0].tolist()\n",
    "    # We initialize empty lists for the 3 regressions that we are going to do of each w_i on the Z kept.\n",
    "    controlvarw1 = []\n",
    "    controlvarw2 = []\n",
    "    controlvarw3 = []\n",
    "\n",
    "    for i in range(len(Z)):\n",
    "        # We fulfill the 3 lists with the non null values remaining of the z_i\n",
    "        controlvarw1.append([Z[i,j] for j in nonnull_w1])\n",
    "        controlvarw2.append([Z[i,j] for j in nonnull_w2])\n",
    "        controlvarw3.append([Z[i,j] for j in nonnull_w3])\n",
    "        \n",
    "    return np.asarray(controlvarw1), np.asarray(controlvarw2), np.asarray(controlvarw3)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Step 4 :*** OLS regression using the parameters we kept, and finding their optimal values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's do the 3 regressions of w_1, w_2 and w_3 on Z with the control variates we kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OLS_reg(coeff_lasso, w1, w2, w3):\n",
    "    controlvarw1, controlvarw2, controlvarw3 = real_extract_non_null(coeff_lasso)\n",
    "    reg1 = LinearRegression()\n",
    "    reg1.fit(controlvarw1, w1) # On fait la regression seulement sur la partie qui n'est pas nulle\n",
    "    coeff_reg1 = reg1.coef_.tolist()\n",
    "    reg2 = LinearRegression()\n",
    "    reg2.fit(controlvarw2, w2)\n",
    "    coeff_reg2 = reg2.coef_.tolist()\n",
    "    reg3 = LinearRegression()\n",
    "    reg3.fit(controlvarw3, w3)\n",
    "    coeff_reg3 = reg3.coef_.tolist()\n",
    "\n",
    "    # We return the coefficient of the 3 regressions\n",
    "    return coeff_reg1, coeff_reg2, coeff_reg3\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets create the matrix as depicted in the paper, the 3x3 matrix, dimension 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def matrix_coeff(coeff_lasso, w1, w2, w3):\n",
    "    coeff_reg1 = OLS_reg(coeff_lasso, w1, w2, w3)[0]\n",
    "    coeff_reg2 = OLS_reg(coeff_lasso, w1, w2, w3)[1]\n",
    "    coeff_reg3 = OLS_reg(coeff_lasso, w1, w2, w3)[2]\n",
    "\n",
    "    # We add 0 to the z_i that were removed after Lasso to have again our 9 control variates \n",
    "    # vectors with some values therefore equal to 0 if the control variate was not used\n",
    "    # Thus we still are in dimension 9 while having removed the useless coefficients\n",
    "\n",
    "    for i in np.where(coeff_lasso[:, 0] == 0)[0]:\n",
    "        coeff_reg1.insert(i,0)\n",
    "    for j in np.where(coeff_lasso[:, 1] == 0)[0]:\n",
    "        coeff_reg2.insert(j,0)\n",
    "    for l in np.where(coeff_lasso[:, 2] == 0)[0]:\n",
    "        coeff_reg3.insert(l,0)\n",
    "        \n",
    "    #This corresponds, for each wi, to the 3 first coefficients of the control variates associated to each linear regression\n",
    "    A_reg1 = [coeff_reg1[i] for i in range(3)]\n",
    "    A_reg2 = [coeff_reg2[i] for i in range(3)]\n",
    "    A_reg3 = [coeff_reg3[i] for i in range(3)]\n",
    "    # direct effects of control variables on the parameter, constant\n",
    "    \n",
    "    # B is symmetric so we only need 6 coefficients for the 6 remaining coefficients of the control variates, for each of the 3 regression\n",
    "    # First regression \n",
    "    reg1_B11 = coeff_reg1[6]\n",
    "    reg1_B12 = coeff_reg1[3]\n",
    "    reg1_B13 = coeff_reg1[5]\n",
    "    reg1_B22 = coeff_reg1[7]\n",
    "    reg1_B23 = coeff_reg1[4]\n",
    "    reg1_B33 = coeff_reg1[8]\n",
    "    B_reg1 = np.array([[reg1_B11,reg1_B12,reg1_B13],[reg1_B12,reg1_B22,reg1_B23],[reg1_B13,reg1_B23,reg1_B33]])\n",
    "    # Coviarance matrix\n",
    "    \n",
    "    # Second regression \n",
    "    reg2_B11 = coeff_reg2[6]\n",
    "    reg2_B12 = coeff_reg2[3]\n",
    "    reg2_B13 = coeff_reg2[5]\n",
    "    reg2_B22 = coeff_reg2[7]\n",
    "    reg2_B23 = coeff_reg2[4]\n",
    "    reg2_B33 = coeff_reg2[8]\n",
    "    B_reg2 = np.array([[reg1_B11,reg1_B12,reg1_B13],[reg1_B12,reg1_B22,reg1_B23],[reg1_B13,reg1_B23,reg1_B33]])\n",
    "    \n",
    "    # Third regression \n",
    "    reg3_11 = coeff_reg3[6]\n",
    "    reg3_B12 = coeff_reg3[3]\n",
    "    reg3_B13 = coeff_reg3[5]\n",
    "    reg3_B22 = coeff_reg3[7]\n",
    "    reg3_B23 = coeff_reg3[4]\n",
    "    reg3_B33 = coeff_reg3[8]\n",
    "    B_reg3 = np.array([[reg1_B11,reg1_B12,reg1_B13],[reg1_B12,reg1_B22,reg1_B23],[reg1_B13,reg1_B23,reg1_B33]])\n",
    "    \n",
    "    return A_reg1, B_reg1, A_reg2, B_reg2, A_reg3, B_reg3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we've obtained the optimal coefficients for the matrix B (symmetric so thats why we have only 6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Step 5 :*** Computing the values and their variance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to create a Trace function as we know the form of the polynom when it's a second order case. \n",
    "Indeed, for quadratic polynomials $P(x) = a^T\\times x + \\frac{1}{2}\\times x^TBx$ (as given in the paper), the re-normalized $\\tilde{f}$ is now:$\\tilde{f}(x) = f(x) - \\frac{1}{2}\\operatorname{tr}(B) + (a + Bx)^Tz$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace(B):\n",
    "    return(B[0,0] + B[1,1] + B[2,2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the estimators of all w now that we added control variates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 feature(s) (shape=(4001, 0)) while a minimum of 1 is required by LinearRegression.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m     w3_tilde \u001b[39m=\u001b[39m [w3[i] \u001b[39m-\u001b[39m (\u001b[39m0.5\u001b[39m)\u001b[39m*\u001b[39mtrace_reg3 \u001b[39m+\u001b[39m np\u001b[39m.\u001b[39mdot(a_reg3 \u001b[39m+\u001b[39m np\u001b[39m.\u001b[39mdot(B_reg3,Y[i]), Z[i]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(Y3))]\n\u001b[1;32m      7\u001b[0m     \u001b[39mreturn\u001b[39;00m w1_tilde, w2_tilde, w3_tilde\n\u001b[0;32m----> 9\u001b[0m w1_tilde, w2_tilde, w3_tilde \u001b[39m=\u001b[39m ZV_MCMC_Q3(coeff_lasso, Y1, Y2, Y3)\n",
      "Cell \u001b[0;32mIn[72], line 2\u001b[0m, in \u001b[0;36mZV_MCMC_Q3\u001b[0;34m(coeff_lasso, w1, w2, w3)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mZV_MCMC_Q3\u001b[39m(coeff_lasso, w1, w2, w3):\n\u001b[0;32m----> 2\u001b[0m     a_reg1, B_reg1, a_reg2, B_reg2, a_reg3, B_reg3 \u001b[39m=\u001b[39m matrix_coeff(coeff_lasso, w1, w2, w3)\n\u001b[1;32m      3\u001b[0m     trace_reg1, trace_reg2, trace_reg3 \u001b[39m=\u001b[39m trace(B_reg1), trace(B_reg2), trace(B_reg3)\n\u001b[1;32m      4\u001b[0m     w1_tilde \u001b[39m=\u001b[39m [w1[i] \u001b[39m-\u001b[39m (\u001b[39m0.5\u001b[39m)\u001b[39m*\u001b[39mtrace_reg1 \u001b[39m+\u001b[39m np\u001b[39m.\u001b[39mdot(a_reg1 \u001b[39m+\u001b[39m np\u001b[39m.\u001b[39mdot(B_reg1,Y[i]), Z[i]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(Y1))]\n",
      "Cell \u001b[0;32mIn[65], line 2\u001b[0m, in \u001b[0;36mmatrix_coeff\u001b[0;34m(coeff_lasso, w1, w2, w3)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmatrix_coeff\u001b[39m(coeff_lasso, w1, w2, w3):\n\u001b[0;32m----> 2\u001b[0m     coeff_reg1 \u001b[39m=\u001b[39m OLS_reg(coeff_lasso, w1, w2, w3)[\u001b[39m0\u001b[39m]\n\u001b[1;32m      3\u001b[0m     coeff_reg2 \u001b[39m=\u001b[39m OLS_reg(coeff_lasso, w1, w2, w3)[\u001b[39m1\u001b[39m]\n\u001b[1;32m      4\u001b[0m     coeff_reg3 \u001b[39m=\u001b[39m OLS_reg(coeff_lasso, w1, w2, w3)[\u001b[39m2\u001b[39m]\n",
      "Cell \u001b[0;32mIn[62], line 4\u001b[0m, in \u001b[0;36mOLS_reg\u001b[0;34m(coeff_lasso, w1, w2, w3)\u001b[0m\n\u001b[1;32m      2\u001b[0m controlvarw1, controlvarw2, controlvarw3 \u001b[39m=\u001b[39m extract_non_null(coeff_lasso)\n\u001b[1;32m      3\u001b[0m reg1 \u001b[39m=\u001b[39m LinearRegression()\n\u001b[0;32m----> 4\u001b[0m reg1\u001b[39m.\u001b[39;49mfit(controlvarw1, w1)\n\u001b[1;32m      5\u001b[0m coeff_reg1 \u001b[39m=\u001b[39m reg1\u001b[39m.\u001b[39mcoef_\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m      6\u001b[0m reg2 \u001b[39m=\u001b[39m LinearRegression()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:678\u001b[0m, in \u001b[0;36mLinearRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    674\u001b[0m n_jobs_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_jobs\n\u001b[1;32m    676\u001b[0m accept_sparse \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpositive \u001b[39melse\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mcsr\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcsc\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcoo\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m--> 678\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m    679\u001b[0m     X, y, accept_sparse\u001b[39m=\u001b[39;49maccept_sparse, y_numeric\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, multi_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m    680\u001b[0m )\n\u001b[1;32m    682\u001b[0m has_sw \u001b[39m=\u001b[39m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    683\u001b[0m \u001b[39mif\u001b[39;00m has_sw:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/base.py:622\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    620\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[1;32m    621\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 622\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    623\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:1146\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         estimator_name \u001b[39m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m   1142\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1143\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m requires y to be passed, but the target y is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1144\u001b[0m     )\n\u001b[0;32m-> 1146\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m   1147\u001b[0m     X,\n\u001b[1;32m   1148\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[1;32m   1149\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49maccept_large_sparse,\n\u001b[1;32m   1150\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   1151\u001b[0m     order\u001b[39m=\u001b[39;49morder,\n\u001b[1;32m   1152\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m   1153\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[1;32m   1154\u001b[0m     ensure_2d\u001b[39m=\u001b[39;49mensure_2d,\n\u001b[1;32m   1155\u001b[0m     allow_nd\u001b[39m=\u001b[39;49mallow_nd,\n\u001b[1;32m   1156\u001b[0m     ensure_min_samples\u001b[39m=\u001b[39;49mensure_min_samples,\n\u001b[1;32m   1157\u001b[0m     ensure_min_features\u001b[39m=\u001b[39;49mensure_min_features,\n\u001b[1;32m   1158\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[1;32m   1159\u001b[0m     input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1160\u001b[0m )\n\u001b[1;32m   1162\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[1;32m   1164\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:976\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    974\u001b[0m     n_features \u001b[39m=\u001b[39m array\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[1;32m    975\u001b[0m     \u001b[39mif\u001b[39;00m n_features \u001b[39m<\u001b[39m ensure_min_features:\n\u001b[0;32m--> 976\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    977\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m feature(s) (shape=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m) while\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    978\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m a minimum of \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m is required\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    979\u001b[0m             \u001b[39m%\u001b[39m (n_features, array\u001b[39m.\u001b[39mshape, ensure_min_features, context)\n\u001b[1;32m    980\u001b[0m         )\n\u001b[1;32m    982\u001b[0m \u001b[39mif\u001b[39;00m copy:\n\u001b[1;32m    983\u001b[0m     \u001b[39mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[1;32m    984\u001b[0m         \u001b[39m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 feature(s) (shape=(4001, 0)) while a minimum of 1 is required by LinearRegression."
     ]
    }
   ],
   "source": [
    "def ZV_MCMC_Q3(coeff_lasso, w1, w2, w3):\n",
    "    a_reg1, B_reg1, a_reg2, B_reg2, a_reg3, B_reg3 = matrix_coeff(coeff_lasso, w1, w2, w3)\n",
    "    trace_reg1, trace_reg2, trace_reg3 = trace(B_reg1), trace(B_reg2), trace(B_reg3)\n",
    "    w1_tilde = [w1[i] - (0.5)*trace_reg1 + np.dot(a_reg1 + np.dot(B_reg1,Y[i]), Z[i]) for i in range(len(Y1))]\n",
    "    w2_tilde = [w2[i] - (0.5)*trace_reg2 + np.dot(a_reg2 + np.dot(B_reg2,Y[i]), Z[i]) for i in range(len(Y2))]\n",
    "    w3_tilde = [w3[i] - (0.5)*trace_reg3 + np.dot(a_reg3 + np.dot(B_reg3,Y[i]), Z[i]) for i in range(len(Y3))]\n",
    "    return w1_tilde, w2_tilde, w3_tilde\n",
    "\n",
    "w1_tilde, w2_tilde, w3_tilde = ZV_MCMC_Q3(coeff_lasso, Y1, Y2, Y3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now compare our new estimation of w with the previous ones, without the addition of control variates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Mean of w_1 for the ZV MCMC method with control variates =\", np.mean(w1_tilde))\n",
    "print(\"Mean of w_1 for the MCMC method without control variates =\", np.mean(w1_samples[:burn_in]))\n",
    "print(\"Variance of w_1 for the ZV MCMC method with control variates =\", np.var(w1_tilde))\n",
    "print(\"Variance of w_1 for the MCMC method without control variates =\", np.var(w1_samples[:burn_in]))\n",
    "\n",
    "print(\"Mean of w_2 for the ZV MCMC method with control variates =\", np.mean(w2_tilde))\n",
    "print(\"Mean of w_2 for the MCMC method without control variates =\", np.mean(w2_samples[:burn_in]))\n",
    "print(\"Variance of w_2 for the ZV MCMC method with control variates =\", np.var(w2_tilde))\n",
    "print(\"Variance of w_2 for the MCMC method with control variates =\", np.var(w2_samples[:burn_in]))\n",
    "\n",
    "print(\"Mean of w_3 for the ZV MCMC method with control variates =\", np.mean(w3_tilde))\n",
    "print(\"Mean of w_3 for the MCMC method with control variates =\", np.mean(w3_samples[:burn_in]))\n",
    "print(\"Variance of w_3 for the ZV MCMC method with control variates =\", np.var(w3_tilde))\n",
    "print(\"Variance of w_3 for the MCMC method with control variates =\", np.var(w3_samples[:burn_in]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now time for comparison with the naive method\n",
    "\n",
    "We could compare this LASSO method with the naive method which only consists in doing the full regression of f on -z in order to find the optimal coefficients 9 components in the vector for the 2nd order polynomial case (instead of 3 in Question 2 with 1st order polynomial case).\n",
    "\n",
    "It should lead to a lower variance but a longer time for the execution of the algorithm, which is why we would prefer use the LASSO Method when we have a high degre polynomial case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we have to compute everything again\n",
    "\n",
    "\n",
    "initial_omega = [0.3, 0.3, 0.3]\n",
    "true_omega = [0.6, 0.7, 0.4]  # These are the true parameters we want to estimate\n",
    "sigma_omega = [9, 9, 9]  # Standard deviations for the priors\n",
    "\n",
    "# Metropolis-Hastings settings\n",
    "iterations = 5000\n",
    "burn_in = 1000\n",
    "sigma_proposal = 0.01\n",
    "\n",
    "# Parameters (example values)\n",
    "omega_1, omega_2, omega_3 =0.1, 0.1, 0.8\n",
    "sigma_1, sigma_2, sigma_3 = 9, 9, 9\n",
    "# Simulate data\n",
    "T = iterations + 1 # Number of time points\n",
    "r, h = simulate_garch(T, true_omega[0], true_omega[1], true_omega[2])\n",
    "\n",
    "# Execute the Metropolis-Hastings algorithm\n",
    "acceptance_rate, omega_samples = metropolis_hasting_algorithm(iterations, r, sigma_proposal, initial_omega, sigma_omega)\n",
    "\n",
    "# We extract the parameter samples\n",
    "omega_samples = np.array(omega_samples)  # Convert list of samples into an array for easier slicing\n",
    "w1_samples = omega_samples[:, 0]\n",
    "w2_samples = omega_samples[:, 1]\n",
    "w3_samples = omega_samples[:, 2]\n",
    "\n",
    "# We compute the log posterior gradients at every point\n",
    "log_post_gradient = compute_log_posterior_gradients_concatenator(r, sigma_1, sigma_2, sigma_3, w1_samples, w2_samples, w3_samples)\n",
    "\n",
    "# We compute the control variates (z_i)\n",
    "x1 = [elem[0] if isinstance(elem, np.ndarray) else elem[0] for elem in log_post_gradient]\n",
    "x2 = [elem[1] if isinstance(elem, np.ndarray) else elem[1] for elem in log_post_gradient]\n",
    "x3 = [elem[2] if isinstance(elem, np.ndarray) else elem[2] for elem in log_post_gradient]\n",
    "\n",
    "x1x2 = list_product(x1, x2)\n",
    "x1x3 = list_product(x1, x3)\n",
    "x2x3 = list_product(x2, x3)\n",
    "x1_square = list_product(x1, x1)\n",
    "x2_square = list_product(x2, x2)\n",
    "x3_square = list_product(x3, x3)\n",
    "\n",
    "# We store these 9 control variates in an array \n",
    "z = np.column_stack((x1, x2, x3, x1x2, x1x3, x2x3, x1_square, x2_square, x3_square))\n",
    "\n",
    "\n",
    "Y = np.array(w[burn_in:])\n",
    "Z = np.array(z[burn_in:])\n",
    "Y1 = np.array(w1_samples[burn_in:])\n",
    "Y2 = np.array(w2_samples[burn_in:])\n",
    "Y3 = np.array(w3_samples[burn_in:])\n",
    "# We do the 3 regressions of w_1, w_2 and w_3 on Z : step 4 of the method \n",
    "\n",
    "def OLS_bis(w1, w2, w3): # New OLS function, that regresses the w_i on all Z, and not a new Z made by Lasso\n",
    "    reg1 = LinearRegression()\n",
    "    reg1.fit(Z, w1) # On fait la regression sur l'enti√©ret√© de Z\n",
    "    coeff_reg1 = reg1.coef_.tolist()\n",
    "    reg2 = LinearRegression()\n",
    "    reg2.fit(Z, w2)\n",
    "    coeff_reg2 = reg2.coef_.tolist()\n",
    "    reg3 = LinearRegression()\n",
    "    reg3.fit(Z, w3)\n",
    "    coeff_reg3 = reg3.coef_.tolist()\n",
    "    # We return the coefficient of the 3 regressions\n",
    "    return coeff_reg1, coeff_reg2, coeff_reg3\n",
    "# We compute the a vector as well as the B matrix defined in the article and previously in the notebook\n",
    "\n",
    "def matrix_coeff_bis(w1, w2, w3):\n",
    "    coeff_reg1 = OLS_bis(w1, w2, w3)[0]\n",
    "    coeff_reg2 = OLS_bis(w1, w2, w3)[1]\n",
    "    coeff_reg3 = OLS_bis(w1, w2, w3)[2]\n",
    "        \n",
    "    #This corresponds, for each wi, to the 3 first coefficients of the control variates associated to each linear regression\n",
    "    a_reg1 = [coeff_reg1[i] for i in range(3)]\n",
    "    a_reg2 = [coeff_reg2[i] for i in range(3)]\n",
    "    a_reg3 = [coeff_reg3[i] for i in range(3)]\n",
    "    \n",
    "    # B is symmetric so we only need 6 coefficients for the 6 remaining coefficients of the control variates, for each of the 3 regression\n",
    "    # First regression \n",
    "    reg1_b11 = coeff_reg1[6]\n",
    "    reg1_b12 = coeff_reg1[3]\n",
    "    reg1_b13 = coeff_reg1[5]\n",
    "    reg1_b22 = coeff_reg1[7]\n",
    "    reg1_b23 = coeff_reg1[4]\n",
    "    reg1_b33 = coeff_reg1[8]\n",
    "    B_reg1 = np.array([[reg1_b11,reg1_b12,reg1_b13],[reg1_b12,reg1_b22,reg1_b23],[reg1_b13,reg1_b23,reg1_b33]])\n",
    "    \n",
    "    # Second regression \n",
    "    reg2_b11 = coeff_reg2[6]\n",
    "    reg2_b12 = coeff_reg2[3]\n",
    "    reg2_b13 = coeff_reg2[5]\n",
    "    reg2_b22 = coeff_reg2[7]\n",
    "    reg2_b23 = coeff_reg2[4]\n",
    "    reg2_b33 = coeff_reg2[8]\n",
    "    B_reg2 = np.array([[reg1_b11,reg1_b12,reg1_b13],[reg1_b12,reg1_b22,reg1_b23],[reg1_b13,reg1_b23,reg1_b33]])\n",
    "    \n",
    "    # Third regression \n",
    "    reg3_b11 = coeff_reg3[6]\n",
    "    reg3_b12 = coeff_reg3[3]\n",
    "    reg3_b13 = coeff_reg3[5]\n",
    "    reg3_b22 = coeff_reg3[7]\n",
    "    reg3_b23 = coeff_reg3[4]\n",
    "    reg3_b33 = coeff_reg3[8]\n",
    "    B_reg3 = np.array([[reg1_b11,reg1_b12,reg1_b13],[reg1_b12,reg1_b22,reg1_b23],[reg1_b13,reg1_b23,reg1_b33]])\n",
    "    \n",
    "    return a_reg1, B_reg1, a_reg2, B_reg2, a_reg3, B_reg3\n",
    "\n",
    "def trace(B):\n",
    "    return(B[0,0] + B[1,1] + B[2,2])\n",
    "\n",
    "def ZV_MCMC_Q3_bis(w1, w2, w3):\n",
    "    a_reg1, B_reg1, a_reg2, B_reg2, a_reg3, B_reg3 = matrix_coeff_bis(w1, w2, w3)\n",
    "    trace_reg1, trace_reg2, trace_reg3 = trace(B_reg1), trace(B_reg2), trace(B_reg3)\n",
    "    w1_tilde = [w1[i] - (0.5)*trace_reg1 + np.dot(a_reg1 + np.dot(B_reg1,Y[i]), x[i]) for i in range(len(Y1))]\n",
    "    w2_tilde = [w2[i] - (0.5)*trace_reg2 + np.dot(a_reg2 + np.dot(B_reg2,Y[i]), x[i]) for i in range(len(Y2))]\n",
    "    w3_tilde = [w3[i] - (0.5)*trace_reg3 + np.dot(a_reg3 + np.dot(B_reg3,Y[i]), x[i]) for i in range(len(Y3))]\n",
    "    return w1_tilde, w2_tilde, w3_tilde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1_tilde_bis, w2_tilde_bis, w3_tilde_bis = ZV_MCMC_Q3_bis(Y1, Y2, Y3)\n",
    "\n",
    "print(\"Mean of w_1 for the ZV MCMC method with control variates =\", np.mean(w1_tilde))\n",
    "print(\"Mean of w_1_bis for the ZV MCMC method with control variates =\", np.mean(w1_tilde_bis))\n",
    "print(\"Mean of w_1 for the MCMC method without control variates =\", np.mean(w1_samples[:burn_in]))\n",
    "print(\"Variance of w_1 for the ZV MCMC method with control variates =\", np.var(w1_tilde))\n",
    "print(\"Variance of w_1 for the ZV MCMC method with control variates =\", np.var(w1_tilde_bis))\n",
    "print(\"Variance of w_1 for the MCMC method without control variates =\", np.var(w1_samples[:burn_in]))\n",
    "\n",
    "print(\"Mean of w_2 for the ZV MCMC method with control variates =\", np.mean(w2_tilde))\n",
    "print(\"Mean of w_2_bis for the ZV MCMC method with control variates =\", np.mean(w2_tilde_bis))\n",
    "print(\"Mean of w_2 for the MCMC method without control variates =\", np.mean(w2_samples[:burn_in]))\n",
    "print(\"Variance of w_2 for the ZV MCMC method with control variates =\", np.var(w2_tilde))\n",
    "print(\"Variance of w_2_bis for the ZV MCMC method with control variates =\", np.var(w2_tilde_bis))\n",
    "print(\"Variance of w_2 for the MCMC method with control variates =\", np.var(w2_samples[:burn_in]))\n",
    "\n",
    "print(\"Mean of w_3 for the ZV MCMC method with control variates =\", np.mean(w3_tilde))\n",
    "print(\"Mean of w_3_bis for the ZV MCMC method with control variates =\", np.mean(w3_tilde_bis))\n",
    "print(\"Mean of w_3 for the MCMC method with control variates =\", np.mean(w3_samples[:burn_in]))\n",
    "print(\"Variance of w_3 for the ZV MCMC method with control variates =\", np.var(w3_tilde))\n",
    "print(\"Variance of w_3_bis for the ZV MCMC method with control variates =\", np.var(w3_tilde_bis))\n",
    "print(\"Variance of w_3 for the MCMC method with control variates =\", np.var(w3_samples[:burn_in]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion : we indeed lower the variance, but it took longer to compute"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
