{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import truncnorm\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "import arch \n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import MultiTaskLasso\n",
    "from sklearn.linear_model import MultiTaskLassoCV"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **III)** Implementation of a method based on Lasso regression in the case of a high degree polynomial (large set if control variates) and comparison with the naive approach *(Question 3)*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First of all, lets explain why, when the number of control variates become too large, the linear regression approach of Step 2 may become too expensive.**\n",
    "\n",
    "First of all, when we consider polynomials, increasing by 1 the degree of the polynomial function increases a lot the number of control variates since, in a $d$-dimensionnal space, a polynomial of order $p$ provides $\\binom{d+p}{d}-1$ control variates according to the first paper. \n",
    "\n",
    "Therefore, for instance in our example where we are in a 3-dimensional space, we effectively have $\\binom{1+3}{1}-1 = \\binom{4}{1}-1 = 3$ control variates that are $z_1, z_2$ and $z_3$ for the first order polynomial case. And, for the 2 order polynomial case, we would have $z_1, z_2, z_3, z_1*z_2, z_3*z_3, z_1*z_3, z_1^2, z_2^2$ and $z_3^2$ which effectively corresponds to  $\\binom{2+3}{2}-1 = \\binom{5}{2}-1 = 9$ control variates. In the same way, we would have $\\binom{3+3}{3}-1 = \\binom{6}{3}-1 = 19$ for the 3rd order polynomial case. \n",
    "\n",
    "On the other hand, the second paper states that the computation time of the OLS method is of the order $nm^2 + m^3 + nt$, where $n$ is the number of samples and $m$ the number of control variates. Indeed, when doing OLS, we need to compute $(ùëã‚Ä≤ùëã)^{‚àí1}ùëã‚Ä≤ùëå$. In general, the complexity of the matrix product $AB$ of two matrices $A$ and $B$ with dimensions $a \\times b$ and $b \\times c$, respectively, can be expressed as $O(abc)$. Therefore, \n",
    "\n",
    "a) The matrix product $X'X$ has a complexity of $O(p^2n)$.\n",
    "\n",
    "b) The matrix-vector product $X'Y$ has a complexity of $O(pn)$.\n",
    "\n",
    "c) The inverse $(X'X)^{-1}$ has a complexity of $O(p^3)$.\n",
    "\n",
    "Thus, the overall complexity of these operations is $O(np^2 + p^3)$.\n",
    "\n",
    "Since we observed a rapid escalation in the number of control variates with higher polynomial degrees, this consequently escalates computational complexity when seeking optimal coefficients through OLS regression. Hence, the second paper proposes a remedy: employing Lasso regression to diminish the count of control variates utilized."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, in statistics and machine learning, LASSO (least absolute shrinkage and selection operator) is a regression analysis method that performs both variable selection and regularization in order to enhance the prediction accuracy and interpretability of the resulting statistical model.\n",
    "\n",
    "It is similar to the OLS but with an additional term which corresponds to lambda, an hyperparameter to find, times the norm 1 of the vector of coefficients. Therefore, in a LASSO regression, we search the parameters that satisfies this : $$\\displaystyle \\min_{\\beta \\in \\mathbb{R}^p} {\\frac{1}{N}} |y - X\\beta|_2^2 + \\lambda |\\beta|_1.$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Therefore, we can adapt the method described in the second paper as the LSLasso method in order to deal with the issue of a large set of control variates. We are going to do the second order polynomial case here.**\n",
    "\n",
    "Indeed, the method that we are going to apply is:\n",
    "\n",
    "**1)** We are first going to find the 9 control variates based on the derivative of ln(posterior) as we did with the 3 control variates in II¬∞ but with a second order polynomial case. We use the gradient function as in question 2 in order to compute $z_1$, $z_2$ and $z_3$, and then, the next 6 control variates that we are going to use are $z_1\\times z_2$, $z_1\\times z_3$, $z_2\\times z_3$, $z_1^2$, $z_2^2$ and $z_3^2$. \n",
    "\n",
    "**2)** Then, we are going to do a LASSO Cross Validation (LassoCV) to regress the $f(w_i)$ on the vector of control variates $[z_i]$ as in Question 2 (but we have 9 control variates instead of 3 so 9 parameters to estimate for each $w_i$) in order to find the optimal hyperparameter $\\lambda$.\n",
    "\n",
    "**3)** Next, armed with the optimal hyperparameter $\\lambda$, we'll conduct LASSO Regression of $f(w_i) = w_i$ on $[z_i]$, employing subsampling to lower computation time. This approach aims to zero out certain coefficients, helping us identify which control variates to retain for the final OLS regression.\n",
    "\n",
    "**4)** With the control variates we kept (after LASSO), we are going to do our OLS regression to find the optimal parameters of the control variates.\n",
    "\n",
    "**5)** Ultimately, we can compute, for instance, the empirical expected value (mean) of each $\\tilde{f}(w_i)$ to determine if we achieve the same expected value as with only $f(w_i)$ but with reduced variance, fulfilling the objective of employing control variates."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***In this question, we are focusing on 2nd order polynomials.***\n",
    " \n",
    " As we know that for quadratic polynomials $P(x) = a^T\\times x + \\frac{1}{2}\\times x^TBx$ (as given in the paper), the re-normalized $\\tilde{f}$ is now:$\\tilde{f}(x) = f(x) - \\frac{1}{2}\\operatorname{tr}(B) + (a + Bx)^Tz$. \n",
    " \n",
    " Here, if we note $\\alpha_1$, $\\alpha_2$, $\\alpha_3$, $\\alpha_{12}$, $\\alpha_{13}$, $\\alpha_{23}$, $\\alpha_{11}$, $\\alpha_{22}$ and $\\alpha_{33}$ the coefficients of $z_1$, $z_2$, $z_3$,$z_1\\times z_2$, $z_1\\times z_3$, $z_2\\times z_3$, $z_1^2$, $z_2^2$ and $z_3^2$, then we have : $a = [\\alpha_1, \\alpha_2, \\alpha_3]$ and \n",
    "$ B = \\begin{pmatrix}\n",
    "\\alpha_{11} & \\alpha_{12} & \\alpha_{13} \\\\\n",
    "\\alpha_{21} & \\alpha_{22} & \\alpha_{23} \\\\\n",
    "\\alpha_{31} & \\alpha_{32} & \\alpha_{33} \\\\\n",
    "\\end{pmatrix}$ but with B symmetric so we only have 6 distinct coefficients in this matrix. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Step 0 :*** Let's remember the functions we'll have to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_garch(T, omega_1, omega_2, omega_3):\n",
    "    np.random.seed(423)  # For reproducibility\n",
    "    r = np.zeros(T)\n",
    "    h = np.zeros(T)\n",
    "    h[0] = omega_1 / (1 - omega_2 - omega_3)\n",
    "    r[0] = np.random.normal(0, np.sqrt(abs(h[0]))) # abs\n",
    "\n",
    "    \n",
    "    for t in range(1, T):\n",
    "        h[t] = omega_1 + omega_2 * r[t-1]**2 + omega_3 * h[t-1]\n",
    "        r[t] = np.random.normal(0, np.sqrt(abs(h[t]))) #abs\n",
    "        \n",
    "    return r, h\n",
    "\n",
    "def log_likelihood(r, omega_1, omega_2, omega_3):\n",
    "    T = len(r)\n",
    "    h = np.zeros(T)\n",
    "    h[0] = omega_1 / (1 - omega_2 - omega_3) if (omega_2 + omega_3) < 1 else 100000  # large number for stability\n",
    "    \n",
    "    log_lik = 0\n",
    "    for t in range(1, T):\n",
    "        h[t] = omega_1 + omega_2 * r[t-1]**2 + omega_3 * h[t-1]\n",
    "        log_lik -= 0.5 * (np.log(abs(h[t])) + r[t]**2 / h[t]) # abs\n",
    "    \n",
    "    return log_lik\n",
    "\n",
    "def log_posterior(r, omega_1, omega_2, omega_3, sigma_1, sigma_2, sigma_3):\n",
    "    log_prior = -0.5 * (omega_1**2 / sigma_1**2 + omega_2**2 / sigma_2**2 + omega_3**2 / sigma_3**2)\n",
    "    return log_likelihood(r, omega_1, omega_2, omega_3) + log_prior\n",
    "\n",
    "def proposal(omega_old, sigma_proposal ): # function used in the MH algorithm that will \"propose\" the new parameter (new omega here)\n",
    "    # omega_old is the list containing the old omegas, we will use a normal probability distribution to determine the new ones\n",
    "    omega = np.array([0.0,0.0,0.0])\n",
    "    omega[0] = np.random.normal(loc= omega_old[0], scale = sigma_proposal)\n",
    "    omega[1] = np.random.normal(loc= omega_old[1], scale = sigma_proposal)\n",
    "    omega[2] = np.random.normal(loc= omega_old[2], scale = sigma_proposal)\n",
    "\n",
    "    return omega\n",
    "\n",
    "def metropolis_hasting_algorithm(iterations, returns, sigma_proposal, initial_omega, sigma_omega):\n",
    "    # Let's initialize our variable\n",
    "    acceptation = np.array([False]*iterations) # we will through the iterations accept or not the change of value of omega\n",
    "    omega = initial_omega\n",
    "    omega_sampling = [omega]\n",
    "    # Let's apply the algorithm trough all the iterations\n",
    "    for t in range(iterations):\n",
    "        # We propose a candidate for omegas\n",
    "        omega_new = proposal(omega, sigma_proposal)\n",
    "        # We calculate the acceptance alpha which is ratio of the prior of the new omega over the old omega\n",
    "        alpha = log_posterior(returns,omega_new[0],omega_new[1],omega_new[2],sigma_omega[0],sigma_omega[1],sigma_omega[2]) - log_posterior(returns,omega[0],omega[1],omega[2],sigma_omega[0],sigma_omega[1],sigma_omega[2])\n",
    "        # We compare alpha with 1 and an uniform random number\n",
    "        u = np.random.uniform(0,1)\n",
    "        if alpha > 0:  # reminder that alpha is a log here so we don't compare with 1 but with 0\n",
    "            acceptation[t] = True\n",
    "            omega = omega_new\n",
    "        if alpha < 0:\n",
    "            if alpha > np.log(u): # we compare to log u and not u\n",
    "                acceptation[t]= False\n",
    "                omega = omega_new # we take the new omega\n",
    "        omega_sampling.append(omega)\n",
    "    acceptance_rate = np.sum(acceptation)/len(acceptation)*100\n",
    "    return acceptance_rate, omega_sampling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_h_t_derivatives(r, omega_1, omega_2, omega_3):\n",
    "    T = len(r)\n",
    "    h = np.zeros(T)\n",
    "    grad_h_omega1 = np.zeros(T)\n",
    "    grad_h_omega2 = np.zeros(T)\n",
    "    grad_h_omega3 = np.zeros(T)\n",
    "\n",
    "    # Initial volatility\n",
    "    h[0] = omega_1  \n",
    "\n",
    "    # Compute h and its derivatives\n",
    "    for t in range(1, T):\n",
    "        h[t] = omega_1 + omega_2 * r[t-1]**2 + omega_3 * h[t-1]\n",
    "        \n",
    "        # Derivative of h_t with respect to omega_1\n",
    "        grad_h_omega1[t] = (1 - omega_3**(t)) / (1 - omega_3) if omega_3 != 1 else t  # Using the geometric series\n",
    "\n",
    "        # Derivative of h_t with respect to omega_2\n",
    "        grad_h_omega2[t] = r[t-1]**2 + omega_3 * grad_h_omega2[t-1]\n",
    "\n",
    "        # Derivative of h_t with respect to omega_3\n",
    "        grad_h_omega3[t] = h[t-1] + omega_3 * grad_h_omega3[t-1]\n",
    "\n",
    "    return h, grad_h_omega1, grad_h_omega2, grad_h_omega3\n",
    "\n",
    "def compute_log_posterior_gradients(r, omega_1, omega_2, omega_3, sigma_1, sigma_2, sigma_3):\n",
    "    h, grad_h_omega1, grad_h_omega2, grad_h_omega3 = compute_h_t_derivatives(r, omega_1, omega_2, omega_3)\n",
    "    T = len(r)\n",
    "    gradients = np.zeros(3)\n",
    "\n",
    "    # Compute the gradient of the log-posterior for each parameter\n",
    "    gradients[0] = -omega_1 / sigma_1**2 - 0.5 * np.sum((1 / h) * grad_h_omega1 - (r**2 / h**2) * grad_h_omega1)\n",
    "    gradients[1] = -omega_2 / sigma_2**2 - 0.5 * np.sum((1 / h) * grad_h_omega2 - (r**2 / h**2) * grad_h_omega2)\n",
    "    gradients[2] = -omega_3 / sigma_3**2 - 0.5 * np.sum((1 / h) * grad_h_omega3 - (r**2 / h**2) * grad_h_omega3)\n",
    "\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_a(f,z):\n",
    "    '''\n",
    "    Calculates the OLS estimator of the vector a\n",
    "\n",
    "    Args : \n",
    "        f (ndarray): Vector containing observed values of the original function f.\n",
    "        z (ndarray): z = (-1/2) * log_post_gradient\n",
    "        \n",
    "    Return : OLS estimator of the vector a\n",
    "    '''\n",
    "    model = LinearRegression()\n",
    "\n",
    "    #Ici on fait une regression lineaire sur toutes nos donnees, est ce qu'il ne faudrait pas virer les 1000 premieres\n",
    "    # par exemple ? (nombre de burn) je suis presque sur que oui\n",
    "    model.fit(-z,f) #performs the linear regression of f on -z\n",
    "    a = model.coef_\n",
    "    return a\n",
    "\n",
    "\n",
    "def alternative_f(f,log_post_gradient):\n",
    "    '''\n",
    "    Calculates the value of the alternative function `f_tilde`\n",
    "    based on the observed values of the original function `f` and log-posterior \n",
    "    gradient.\n",
    "\n",
    "    Args:\n",
    "        f (ndarray): Vector containing observed values of the original function f.\n",
    "        log_post_gradient (ndarray): The log-posterior gradient.\n",
    "\n",
    "    Return:\n",
    "        (ndarray): Vector of values of f_tilde \n",
    "    '''\n",
    "    z = (-1/2) * log_post_gradient\n",
    "    a = optimal_a(f,z)\n",
    "    z = z.T\n",
    "    return f.T + np.dot(a.T,z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_posterior_gradients_concatenator(r, sigma_1, sigma_2, sigma_3,w1_samples,w2_samples,w3_samples):\n",
    "    '''\n",
    "    This function computes the log posterior gradients at every points\n",
    "    '''\n",
    "    gradient_concat=[]\n",
    "    i = 0\n",
    "    for _ in r:\n",
    "        gradient_concat.append(compute_log_posterior_gradients(r, w1_samples[i],w2_samples[i],w3_samples[i], sigma_1, sigma_2, sigma_3))\n",
    "        i = i +1 \n",
    "    return np.array(gradient_concat)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's define a new function that will help us calculte the 6 missing control variates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_product(x,y):\n",
    "    result = []\n",
    "    # The 2 lists must be of same length\n",
    "    for i in range(len(x)):\n",
    "        product = x[i] * y[i]\n",
    "        result.append(product)\n",
    "    return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Step 1 :*** Compute the 6 new control variates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "initial_omega = [0.3, 0.3, 0.3]\n",
    "true_omega = [0.6, 0.7, 0.4]  # These are the true parameters we want to estimate\n",
    "sigma_omega = [9, 9, 9]  # Standard deviations for the priors\n",
    "\n",
    "# Simulate data\n",
    "returns, _ = simulate_garch(2000, *true_omega)\n",
    "\n",
    "# Metropolis-Hastings settings\n",
    "iterations = 5000\n",
    "burn_in = 1000\n",
    "sigma_proposal = 0.01\n",
    "\n",
    "# Parameters (example values)\n",
    "omega_1, omega_2, omega_3 =0.1, 0.1, 0.8\n",
    "sigma_1, sigma_2, sigma_3 = 9, 9, 9\n",
    "# Simulate data\n",
    "T = iterations + 1 # Number of time points\n",
    "r, h = simulate_garch(T, true_omega[0], true_omega[1], true_omega[2])\n",
    "\n",
    "# Execute the Metropolis-Hastings algorithm\n",
    "acceptance_rate, omega_samples = metropolis_hasting_algorithm(iterations, returns, sigma_proposal, initial_omega, sigma_omega)\n",
    "\n",
    "# We extract the parameter samples\n",
    "omega_samples = np.array(omega_samples)  # Convert list of samples into an array for easier slicing\n",
    "w1_samples = omega_samples[:, 0]\n",
    "w2_samples = omega_samples[:, 1]\n",
    "w3_samples = omega_samples[:, 2]\n",
    "\n",
    "# We compute the log posterior gradients at every point\n",
    "log_post_gradient = compute_log_posterior_gradients_concatenator(returns, sigma_1, sigma_2, sigma_3, w1_samples, w2_samples, w3_samples)\n",
    "\n",
    "# We compute the control variates (z_i)\n",
    "x1 = [elem[0] if isinstance(elem, np.ndarray) else elem[0] for elem in log_post_gradient]\n",
    "x2 = [elem[1] if isinstance(elem, np.ndarray) else elem[1] for elem in log_post_gradient]\n",
    "x3 = [elem[2] if isinstance(elem, np.ndarray) else elem[2] for elem in log_post_gradient]\n",
    "\n",
    "x1x2 = list_product(x1, x2)\n",
    "x1x3 = list_product(x1, x3)\n",
    "x2x3 = list_product(x2, x3)\n",
    "x1_square = list_product(x1, x1)\n",
    "x2_square = list_product(x2, x2)\n",
    "x3_square = list_product(x3, x3)\n",
    "\n",
    "# We store these 9 control variates in an array \n",
    "z = np.column_stack((x1, x2, x3, x1x2, x1x3, x2x3, x1_square, x2_square, x3_square))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.59059542e+03, 4.50490550e+03, 4.02741211e+03, ...,\n",
       "        2.52999380e+06, 2.02941736e+07, 1.62200483e+07],\n",
       "       [1.59059542e+03, 4.50490550e+03, 4.02741211e+03, ...,\n",
       "        2.52999380e+06, 2.02941736e+07, 1.62200483e+07],\n",
       "       [1.59059542e+03, 4.50490550e+03, 4.02741211e+03, ...,\n",
       "        2.52999380e+06, 2.02941736e+07, 1.62200483e+07],\n",
       "       ...,\n",
       "       [1.02023019e+01, 1.71380460e+02, 2.32974183e+02, ...,\n",
       "        1.04086964e+02, 2.93712622e+04, 5.42769702e+04],\n",
       "       [1.02023019e+01, 1.71380460e+02, 2.32974183e+02, ...,\n",
       "        1.04086964e+02, 2.93712622e+04, 5.42769702e+04],\n",
       "       [1.02023019e+01, 1.71380460e+02, 2.32974183e+02, ...,\n",
       "        1.04086964e+02, 2.93712622e+04, 5.42769702e+04]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We standardize or not our control variates, as it is usually done in a Lasso Regression. \n",
    "\n",
    "#scaler = StandardScaler()\n",
    "#Z = scaler.fit_transform(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.column_stack((w1, w2, w3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We store our data in arrays after burn in \n",
    "Y = np.array(w[burn_in:])\n",
    "Z = np.array(z[burn_in:])\n",
    "# We also store \n",
    "Y1 = np.array(w1[burn_in:])\n",
    "Y2 = np.array(w2[burn_in:])\n",
    "Y3 = np.array(w3[burn_in:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code ann√©e derni√®re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_omega = [0.3,0.3,0.3]\n",
    "\n",
    "# We simulate our data following a Garch(1,1) with parameters w1, w2 and w3 fixed to check if we have a close expeted value\n",
    "returns = simulate_garch(omega = [0.1, 0.2, 0.7], T = 1000)[0]\n",
    "\n",
    "iterations = 5000\n",
    "burn_in = 1000\n",
    "\n",
    "std_prior = 10\n",
    "std_proposal = 0.01\n",
    "# We run the basic Metropolis Sampler\n",
    "liste = metropolis_hastings(initial_omega, returns, iterations,std_prior, std_proposal)\n",
    "\n",
    "# We store the values of w1, w2 and w3 \n",
    "w1 = [elem[0] if isinstance(elem, np.ndarray) else elem[0] for elem in liste[0]]\n",
    "w2 = [elem[1] if isinstance(elem, np.ndarray) else elem[1] for elem in liste[0]]\n",
    "w3 = [elem[2] if isinstance(elem, np.ndarray) else elem[2] for elem in liste[0]]\n",
    "\n",
    "# We create a vector omega \n",
    "w = np.column_stack((w1, w2, w3))\n",
    "\n",
    "# We compute the gradient for each w = [w1,w2,w3] generated in the Chain thanks to our gradient function\n",
    "x = [gradient(_post, np.asarray(omega)) for omega in w]\n",
    "\n",
    "# We store the gradients and we multiply by 0.5 like in the formula of the article\n",
    "x1 = [elem[0] if isinstance(elem, np.ndarray) else elem[0] for elem in x]\n",
    "x1 = [0.5*x for x in x1]\n",
    "\n",
    "x2 = [elem[1] if isinstance(elem, np.ndarray) else elem[1] for elem in x]\n",
    "x2 = [0.5*x for x in x2]\n",
    "\n",
    "x3 = [elem[2] if isinstance(elem, np.ndarray) else elem[2] for elem in x]\n",
    "x3 = [0.5*x for x in x3]\n",
    "\n",
    "# We construct the 6 other control variates thanks to the function list_product\n",
    "x1x2 = list_product(x1,x2)\n",
    "x1x3 = list_product(x1,x3)\n",
    "x2x3 = list_product(x2,x3)\n",
    "x1_square = list_product(x1,x1)\n",
    "x2_square = list_product(x2,x2)\n",
    "x3_square = list_product(x3,x3)\n",
    "\n",
    "# We store these 9 control variates in an array \n",
    "z = np.column_stack((x1, x2, x3, x1x2, x2x3, x1x3, x1_square, x2_square, x3_square))\n",
    "\n",
    "# We store our data in arrays after burn in \n",
    "Y = np.array(w[burn_in:])\n",
    "Z = np.array(z[burn_in:])\n",
    "# We also store \n",
    "Y1 = np.array(w1[burn_in:])\n",
    "Y2 = np.array(w2[burn_in:])\n",
    "Y3 = np.array(w3[burn_in:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Step 2 :*** Lasso cross validation to find the best hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [0.0001, 0.0001, 0.001, 0.1, 1,1.5]\n",
    "\n",
    "# We do a Lasso Cross Validation in order to find the optimal parameter lambda : step 2 of the method\n",
    "lasso_cv = LassoCV(cv=5)\n",
    "\n",
    "lasso_cv.fit(Z,Y1)\n",
    "\n",
    "print(\"Optimal lambda:\", lasso_cv.alpha_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the optimal lambda, we do a Lasso regression to select the variables : Step 3 of the method\n",
    "lasso_bestmodel = Lasso(alpha=lasso_cv.alpha_)\n",
    "lasso_bestmodel.fit(Y,Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_lasso = lasso_bestmodel.coef_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Step 3 :*** Lasso regression using the parameters we found, and only using the most useful control variates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def controlvar(coeff_lasso):\n",
    "    # Indexes of the non nul coefficients in the first, second and third column\n",
    "    indices_controlvarw1 = np.nonzero(coeff_lasso[:, 0])[0].tolist()\n",
    "    indices_controlvarw2 = np.nonzero(coeff_lasso[:, 1])[0].tolist()\n",
    "    indices_controlvarw3 = np.nonzero(coeff_lasso[:, 2])[0].tolist()\n",
    "    # We initialize empty lists for the 3 regressions that we are going to do of each w_i on the Z kept.\n",
    "    controlvarw1 = []\n",
    "    controlvarw2 = []\n",
    "    controlvarw3 = []\n",
    "    for i in range(len(Z)):\n",
    "        # We fulfill the 3 lists with the non null values remaining of the z_i\n",
    "        controlvarw1.append([Z[i,j] for j in indices_controlvarw1])\n",
    "        controlvarw2.append([Z[i,j] for j in indices_controlvarw2])\n",
    "        controlvarw3.append([Z[i,j] for j in indices_controlvarw3])\n",
    "    return np.asarray(controlvarw1), np.asarray(controlvarw2), np.asarray(controlvarw3)\n",
    "# We do the 3 regressions of w_1, w_2 and w_3 on Z : step 4 of the method \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Step 4 :*** OLS regression using the parameters we kept, and finding their optimal values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def OLS(coeff_lasso, w1, w2, w3):\n",
    "    controlvarw1, controlvarw2, controlvarw3 = controlvar(coeff_lasso)\n",
    "    reg1 = LinearRegression()\n",
    "    reg1.fit(controlvarw1, w1)\n",
    "    coeff_reg1 = reg1.coef_.tolist()\n",
    "    reg2 = LinearRegression()\n",
    "    reg2.fit(controlvarw2, w2)\n",
    "    coeff_reg2 = reg2.coef_.tolist()\n",
    "    reg3 = LinearRegression()\n",
    "    reg3.fit(controlvarw3, w3)\n",
    "    coeff_reg3 = reg3.coef_.tolist()\n",
    "    # We return the coefficient of the 3 regressions\n",
    "    return coeff_reg1, coeff_reg2, coeff_reg3\n",
    "# We compute the a vector as well as the B matrix defined in the article and previously in the notebook\n",
    "\n",
    "def coefficients_finaux(coeff_lasso, w1, w2, w3):\n",
    "    coeff_reg1 = OLS(coeff_lasso, w1, w2, w3)[0]\n",
    "    coeff_reg2 = OLS(coeff_lasso, w1, w2, w3)[1]\n",
    "    coeff_reg3 = OLS(coeff_lasso, w1, w2, w3)[2]\n",
    "    # We add 0 to the z_i that were removed after Lasso to have again our 9 control variates \n",
    "    # vectors with some values therefore equal to 0 if thecontrol variate was not used\n",
    "    for i in np.where(coeff_lasso[:, 0] == 0)[0]:\n",
    "        coeff_reg1.insert(i,0)\n",
    "    for j in np.where(coeff_lasso[:, 1] == 0)[0]:\n",
    "        coeff_reg2.insert(j,0)\n",
    "    for l in np.where(coeff_lasso[:, 2] == 0)[0]:\n",
    "        coeff_reg2.insert(l,0)\n",
    "        \n",
    "    #This corresponds, for each wi, to the 3 first coefficients of the control variates associated to each linear regression\n",
    "    a_reg1 = [coeff_reg1[i] for i in range(3)]\n",
    "    a_reg2 = [coeff_reg2[i] for i in range(3)]\n",
    "    a_reg3 = [coeff_reg3[i] for i in range(3)]\n",
    "    \n",
    "    # B is symmetric so we only need 6 coefficients for the 6 remaining coefficients of the control variates, for each of the 3 regression\n",
    "    # First regression \n",
    "    reg1_b11 = coeff_reg1[6]\n",
    "    reg1_b12 = coeff_reg1[3]\n",
    "    reg1_b13 = coeff_reg1[5]\n",
    "    reg1_b22 = coeff_reg1[7]\n",
    "    reg1_b23 = coeff_reg1[4]\n",
    "    reg1_b33 = coeff_reg1[8]\n",
    "    B_reg1 = np.array([[reg1_b11,reg1_b12,reg1_b13],[reg1_b12,reg1_b22,reg1_b23],[reg1_b13,reg1_b23,reg1_b33]])\n",
    "    \n",
    "    # Second regression \n",
    "    reg2_b11 = coeff_reg2[6]\n",
    "    reg2_b12 = coeff_reg2[3]\n",
    "    reg2_b13 = coeff_reg2[5]\n",
    "    reg2_b22 = coeff_reg2[7]\n",
    "    reg2_b23 = coeff_reg2[4]\n",
    "    reg2_b33 = coeff_reg2[8]\n",
    "    B_reg2 = np.array([[reg1_b11,reg1_b12,reg1_b13],[reg1_b12,reg1_b22,reg1_b23],[reg1_b13,reg1_b23,reg1_b33]])\n",
    "    \n",
    "    # Third regression \n",
    "    reg3_b11 = coeff_reg3[6]\n",
    "    reg3_b12 = coeff_reg3[3]\n",
    "    reg3_b13 = coeff_reg3[5]\n",
    "    reg3_b22 = coeff_reg3[7]\n",
    "    reg3_b23 = coeff_reg3[4]\n",
    "    reg3_b33 = coeff_reg3[8]\n",
    "    B_reg3 = np.array([[reg1_b11,reg1_b12,reg1_b13],[reg1_b12,reg1_b22,reg1_b23],[reg1_b13,reg1_b23,reg1_b33]])\n",
    "    \n",
    "    return a_reg1, B_reg1, a_reg2, B_reg2, a_reg3, B_reg3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Step 5 :*** Computing the values and their variance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to create a Trace function as we know the form of the polynom when it's a second order case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace(B):\n",
    "    return(B[0,0] + B[1,1] + B[2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ZV_MCMC_Q3(coeff_lasso, w1, w2, w3):\n",
    "    a_reg1, B_reg1, a_reg2, B_reg2, a_reg3, B_reg3 = coefficients_finaux(coeff_lasso, w1, w2, w3)\n",
    "    trace_reg1, trace_reg2, trace_reg3 = trace(B_reg1), trace(B_reg2), trace(B_reg3)\n",
    "    w1_tilde = [w1[i] - (0.5)*trace_reg1 + np.dot(a_reg1 + np.dot(B_reg1,Y[i]), x[i]) for i in range(len(Y1))]\n",
    "    w2_tilde = [w2[i] - (0.5)*trace_reg2 + np.dot(a_reg2 + np.dot(B_reg2,Y[i]), x[i]) for i in range(len(Y2))]\n",
    "    w3_tilde = [w3[i] - (0.5)*trace_reg3 + np.dot(a_reg3 + np.dot(B_reg3,Y[i]), x[i]) for i in range(len(Y3))]\n",
    "    return w1_tilde, w2_tilde, w3_tilde\n",
    "\n",
    "w1_tilde, w2_tilde, w3_tilde = ZV_MCMC_Q3(coeff_lasso, Y1, Y2, Y3)\n",
    "\n",
    "print(\"Mean of w_1 for the ZV MCMC method with control variates =\", np.mean(w1_tilde))\n",
    "print(\"Mean of w_1 for the MCMC method without control variates =\", np.mean(w1[:burn_in]))\n",
    "print(\"Variance of w_1 for the ZV MCMC method with control variates =\", np.var(w1_tilde))\n",
    "print(\"Variance of w_1 for the MCMC method without control variates =\", np.var(w1[:burn_in]))\n",
    "\n",
    "print(\"Mean of w_2 for the ZV MCMC method with control variates =\", np.mean(w2_tilde))\n",
    "print(\"Mean of w_2 for the MCMC method without control variates =\", np.mean(w2[:burn_in]))\n",
    "print(\"Variance of w_2 for the ZV MCMC method with control variates =\", np.var(w2_tilde))\n",
    "print(\"Variance of w_2 for the MCMC method with control variates =\", np.var(w2[:burn_in]))\n",
    "\n",
    "\n",
    "print(\"Mean of w_3 for the ZV MCMC method with control variates =\", np.mean(w3_tilde))\n",
    "print(\"Mean of w_3 for the MCMC method with control variates =\", np.mean(w3[:burn_in]))\n",
    "print(\"Variance of w_3 for the ZV MCMC method with control variates =\", np.var(w3_tilde))\n",
    "print(\"Variance of w_3 for the MCMC method with control variates =\", np.var(w3[:burn_in]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4287505587.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[15], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    boxplot_comparision(w3_tilde,w3[burn_in:], 'w3')x\u001b[0m\n\u001b[0m                                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Visualisation \n",
    "\n",
    "\"\"\"\n",
    "boxplot_comparision(w1_tilde, w1[burn_in:],'w1')\n",
    "boxplot_comparision( w2_tilde, w2[burn_in:], 'w2')\n",
    "boxplot_comparision(w3_tilde,w3[burn_in:], 'w3')x\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now time for comparison with the naive method\n",
    "\n",
    "We could compare this LASSO method with the naive method which only consists in doing the full regression of f on -z in order to find the optimal coefficients 9 components in the vector for the 2nd order polynomial case (instead of 3 in Question 2 with 1st order polynomial case).\n",
    "\n",
    "It should lead to a lower variance but a longer time for the execution of the algorithm, which is why we would prefer use the LASSO Method when we have a high degre polynomial case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_omega = [0.3,0.3,0.3]\n",
    "\n",
    "# We simulate our data following a Garch(1,1) with parameters w1, w2 and w3 fixed to check if we have a close expeted value\n",
    "returns = simulate_garch(omega = [0.1, 0.2, 0.7], T = 1000)[0]\n",
    "\n",
    "iterations = 5000\n",
    "burn_in = 1000\n",
    "\n",
    "std_prior = 10\n",
    "std_proposal = 0.01\n",
    "# We run the basic Metropolis Sampler\n",
    "liste = metropolis_hastings(initial_omega, returns, iterations,std_prior, std_proposal)\n",
    "\n",
    "# We store the values of w1, w2 and w3 \n",
    "w1 = [elem[0] if isinstance(elem, np.ndarray) else elem[0] for elem in liste[0]]\n",
    "w2 = [elem[1] if isinstance(elem, np.ndarray) else elem[1] for elem in liste[0]]\n",
    "w3 = [elem[2] if isinstance(elem, np.ndarray) else elem[2] for elem in liste[0]]\n",
    "\n",
    "# We create a vector omega \n",
    "w = np.column_stack((w1, w2, w3))\n",
    "\n",
    "# We compute the gradient for each w = [w1,w2,w3] generated in the Chain thanks to our gradient function\n",
    "x = [gradient(_post, np.asarray(omega)) for omega in w]\n",
    "\n",
    "# We store the gradients and we multiply by 0.5 like in the formula of the article\n",
    "x1 = [elem[0] if isinstance(elem, np.ndarray) else elem[0] for elem in x]\n",
    "x1 = [0.5*x for x in x1]\n",
    "\n",
    "x2 = [elem[1] if isinstance(elem, np.ndarray) else elem[1] for elem in x]\n",
    "x2 = [0.5*x for x in x2]\n",
    "\n",
    "x3 = [elem[2] if isinstance(elem, np.ndarray) else elem[2] for elem in x]\n",
    "x3 = [0.5*x for x in x3]\n",
    "\n",
    "# We construct the 6 other control variates thanks to the function list_product\n",
    "x1x2 = list_product(x1,x2)\n",
    "x1x3 = list_product(x1,x3)\n",
    "x2x3 = list_product(x2,x3)\n",
    "x1_square = list_product(x1,x1)\n",
    "x2_square = list_product(x2,x2)\n",
    "x3_square = list_product(x3,x3)\n",
    "\n",
    "# We store these 9 control variates in an array \n",
    "z = np.column_stack((x1, x2, x3, x1x2, x2x3, x1x3, x1_square, x2_square, x3_square))\n",
    "\n",
    "# We store our data in arrays after burn in \n",
    "Y = np.array(w[burn_in:])\n",
    "Z = np.array(z[burn_in:])\n",
    "# We also store \n",
    "Y1 = np.array(w1[burn_in:])\n",
    "Y2 = np.array(w2[burn_in:])\n",
    "Y3 = np.array(w3[burn_in:])\n",
    "# We do the 3 regressions of w_1, w_2 and w_3 on Z : step 4 of the method \n",
    "\n",
    "def OLS_bis(w1, w2, w3):\n",
    "    reg1 = LinearRegression()\n",
    "    reg1.fit(Z, w1)\n",
    "    coeff_reg1 = reg1.coef_.tolist()\n",
    "    reg2 = LinearRegression()\n",
    "    reg2.fit(Z, w2)\n",
    "    coeff_reg2 = reg2.coef_.tolist()\n",
    "    reg3 = LinearRegression()\n",
    "    reg3.fit(Z, w3)\n",
    "    coeff_reg3 = reg3.coef_.tolist()\n",
    "    # We return the coefficient of the 3 regressions\n",
    "    return coeff_reg1, coeff_reg2, coeff_reg3\n",
    "# We compute the a vector as well as the B matrix defined in the article and previously in the notebook\n",
    "\n",
    "def coefficients_finaux_bis(w1, w2, w3):\n",
    "    coeff_reg1 = OLS_bis(w1, w2, w3)[0]\n",
    "    coeff_reg2 = OLS_bis(w1, w2, w3)[1]\n",
    "    coeff_reg3 = OLS_bis(w1, w2, w3)[2]\n",
    "        \n",
    "    #This corresponds, for each wi, to the 3 first coefficients of the control variates associated to each linear regression\n",
    "    a_reg1 = [coeff_reg1[i] for i in range(3)]\n",
    "    a_reg2 = [coeff_reg2[i] for i in range(3)]\n",
    "    a_reg3 = [coeff_reg3[i] for i in range(3)]\n",
    "    \n",
    "    # B is symmetric so we only need 6 coefficients for the 6 remaining coefficients of the control variates, for each of the 3 regression\n",
    "    # First regression \n",
    "    reg1_b11 = coeff_reg1[6]\n",
    "    reg1_b12 = coeff_reg1[3]\n",
    "    reg1_b13 = coeff_reg1[5]\n",
    "    reg1_b22 = coeff_reg1[7]\n",
    "    reg1_b23 = coeff_reg1[4]\n",
    "    reg1_b33 = coeff_reg1[8]\n",
    "    B_reg1 = np.array([[reg1_b11,reg1_b12,reg1_b13],[reg1_b12,reg1_b22,reg1_b23],[reg1_b13,reg1_b23,reg1_b33]])\n",
    "    \n",
    "    # Second regression \n",
    "    reg2_b11 = coeff_reg2[6]\n",
    "    reg2_b12 = coeff_reg2[3]\n",
    "    reg2_b13 = coeff_reg2[5]\n",
    "    reg2_b22 = coeff_reg2[7]\n",
    "    reg2_b23 = coeff_reg2[4]\n",
    "    reg2_b33 = coeff_reg2[8]\n",
    "    B_reg2 = np.array([[reg1_b11,reg1_b12,reg1_b13],[reg1_b12,reg1_b22,reg1_b23],[reg1_b13,reg1_b23,reg1_b33]])\n",
    "    \n",
    "    # Third regression \n",
    "    reg3_b11 = coeff_reg3[6]\n",
    "    reg3_b12 = coeff_reg3[3]\n",
    "    reg3_b13 = coeff_reg3[5]\n",
    "    reg3_b22 = coeff_reg3[7]\n",
    "    reg3_b23 = coeff_reg3[4]\n",
    "    reg3_b33 = coeff_reg3[8]\n",
    "    B_reg3 = np.array([[reg1_b11,reg1_b12,reg1_b13],[reg1_b12,reg1_b22,reg1_b23],[reg1_b13,reg1_b23,reg1_b33]])\n",
    "    \n",
    "    return a_reg1, B_reg1, a_reg2, B_reg2, a_reg3, B_reg3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace(B):\n",
    "    return(B[0,0] + B[1,1] + B[2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ZV_MCMC_Q3_bis(w1, w2, w3):\n",
    "    a_reg1, B_reg1, a_reg2, B_reg2, a_reg3, B_reg3 = coefficients_finaux_bis(w1, w2, w3)\n",
    "    trace_reg1, trace_reg2, trace_reg3 = trace(B_reg1), trace(B_reg2), trace(B_reg3)\n",
    "    w1_tilde = [w1[i] - (0.5)*trace_reg1 + np.dot(a_reg1 + np.dot(B_reg1,Y[i]), x[i]) for i in range(len(Y1))]\n",
    "    w2_tilde = [w2[i] - (0.5)*trace_reg2 + np.dot(a_reg2 + np.dot(B_reg2,Y[i]), x[i]) for i in range(len(Y2))]\n",
    "    w3_tilde = [w3[i] - (0.5)*trace_reg3 + np.dot(a_reg3 + np.dot(B_reg3,Y[i]), x[i]) for i in range(len(Y3))]\n",
    "    return w1_tilde, w2_tilde, w3_tilde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1_tilde, w2_tilde, w3_tilde = ZV_MCMC_Q3_bis(Y1, Y2, Y3)\n",
    "\n",
    "print(\"Mean of w_1 for the ZV MCMC method with control variates =\", np.mean(w1_tilde))\n",
    "print(\"Mean of w_1 for the MCMC method without control variates =\", np.mean(w1[:burn_in]))\n",
    "print(\"Variance of w_1 for the ZV MCMC method with control variates =\", np.var(w1_tilde))\n",
    "print(\"Variance of w_1 for the MCMC method without control variates =\", np.var(w1[:burn_in]))\n",
    "\n",
    "print(\"Mean of w_2 for the ZV MCMC method with control variates =\", np.mean(w2_tilde))\n",
    "print(\"Mean of w_2 for the MCMC method without control variates =\", np.mean(w2[:burn_in]))\n",
    "print(\"Variance of w_2 for the ZV MCMC method with control variates =\", np.var(w2_tilde))\n",
    "print(\"Variance of w_2 for the MCMC method with control variates =\", np.var(w2[:burn_in]))\n",
    "\n",
    "print(\"Mean of w_3 for the ZV MCMC method with control variates =\", np.mean(w3_tilde))\n",
    "print(\"Mean of w_3 for the MCMC method with control variates =\", np.mean(w3[:burn_in]))\n",
    "print(\"Variance of w_3 for the ZV MCMC method with control variates =\", np.var(w3_tilde))\n",
    "print(\"Variance of w_3 for the MCMC method with control variates =\", np.var(w3[:burn_in]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion : we indeed lower ccomputation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
